{
    "300": {
        "file_id": 16,
        "content": "                    flag = True\n                    break\n            else:\n                if e.lower() in n.lower():\n                    flag = True\n                    break\n        if not flag:\n            p.requires_grad_(False)\n        else:\n            total_trainable += p.numel()\n            if 'encoder' in n or 'vit' in n:\n                p.lr_scale = 0.1\n            print_rank0(n)\n    print_rank0(\"***** Total trainable parameters: \"+str(total_trainable)+\" *****\")\nFineTuneTrainCogAgentModel.disable_untrainable_params = disable_untrainable_params\ndef data_collator(examples, cross_image_processor=None):\n    def to_tensor(value):\n        \"\"\"Converts lists or numpy arrays to tensors.\"\"\"\n        if isinstance(value, list):\n            return torch.tensor(value)\n        elif isinstance(value, np.ndarray):\n            return torch.from_numpy(value)\n        return value\n    def concatenate_tensors(attribute, key):\n        \"\"\"Concatenates tensors for a specific attribute and key.\"\"\"\n        if attribute is None:",
        "type": "code",
        "location": "/finetune_demo/finetune_cogagent_demo.py:27-55"
    },
    "301": {
        "file_id": 16,
        "content": "The code is iterating through the parameters and checking if they belong to specific modules, such as 'encoder' or 'vit'. If a parameter belongs to one of these modules, it will set its learning rate scale to 0.1. It also counts the total number of trainable parameters and prints the count at the end. The function `data_collator` is responsible for converting lists or numpy arrays to tensors and concatenating tensors for specific attributes.",
        "type": "comment"
    },
    "302": {
        "file_id": 16,
        "content": "            return torch.cat([ex[key] for ex in examples if isinstance(ex[key], torch.Tensor)])\n        else:\n            return torch.cat([ex[attribute][key] for ex in examples if isinstance(ex[attribute][key], torch.Tensor)])\n    # Convert all lists and numpy arrays in examples to tensors\n    for example in examples:\n        for key, value in example.items():\n            example[key] = to_tensor(value)\n    # Extract and concatenate attributes from examples\n    img_args = {}\n    for attribute in ['vision', 'cross']:\n        if attribute == 'cross' and cross_image_processor is None:\n            continue\n        if attribute in examples[-1]:  # Using the last example as reference\n            for key in examples[-1][attribute]:\n                tensor_key = f\"{attribute}_{key}\"\n                tensors_to_concatenate = [ex[attribute][key] for ex in examples if isinstance(ex[attribute][key], torch.Tensor)]\n                if tensors_to_concatenate:\n                    img_args[tensor_key] = concatenate_tensors(attribute, key)",
        "type": "code",
        "location": "/finetune_demo/finetune_cogagent_demo.py:56-76"
    },
    "303": {
        "file_id": 16,
        "content": "This code converts all lists and numpy arrays in examples to tensors, then extracts and concatenates attributes from examples.",
        "type": "comment"
    },
    "304": {
        "file_id": 16,
        "content": "                else:\n                    img_args[tensor_key] = examples[-1][attribute][key]\n    # Remove 'vision' and 'cross' keys from examples\n    for example in examples:\n        example.pop('vision', None)\n        example.pop('cross', None)\n    # Create model_args by concatenating tensors and copying other attributes\n    model_args = {key: concatenate_tensors(None, key) \n                  if isinstance(examples[-1][key], torch.Tensor) else examples[-1][key] \n                  for key in examples[-1]\n                  }\n    # Merge img_args into model_args\n    model_args.update(img_args)\n    return model_args\nfrom collections import defaultdict\ndef broadcast_auto(data_dict):\n    type2list = defaultdict(list)\n    other = []\n    for k in data_dict:\n        if type(data_dict[k]) is torch.Tensor:\n            type2list[data_dict[k].dtype].append(k)\n        else:\n            other.append(k)\n    new_data = {}\n    for k in type2list:\n        new_data.update(mpu.broadcast_data(type2list[k], data_dict, k))\n    for k in other:",
        "type": "code",
        "location": "/finetune_demo/finetune_cogagent_demo.py:77-109"
    },
    "305": {
        "file_id": 16,
        "content": "This code is part of a function that finetunes a COG Agent model. It first checks if a tensor key exists in the last example, and if not, assigns it from there. Then, it removes 'vision' and 'cross' keys from all examples. Next, it creates `model_args` by concatenating tensors and copying other attributes from the last example for all keys in that example. Finally, it merges `img_args` into `model_args`. The code also includes a separate function (`broadcast_auto`) that seems to handle broadcasting data based on its type.",
        "type": "comment"
    },
    "306": {
        "file_id": 16,
        "content": "        new_data[k] = data_dict[k]\n    return new_data\ndef get_batch(data_iterator, args, timers):\n    # Broadcast data.\n    timers('data loader').start()\n    if data_iterator is not None:\n        data = next(data_iterator)\n    else:\n        data = None\n    timers('data loader').stop()\n    data_b = broadcast_auto(data)\n    for k in data_b:\n        if type(data_b[k]) is torch.Tensor and data_b[k].dtype is not torch.int32 and data_b[k].dtype is not torch.long:\n            if args.fp16:\n                data_b[k] = data_b[k].half()\n            elif args.bf16:\n                data_b[k] = data_b[k].bfloat16()\n    return data_b\nfrom torch.nn import CrossEntropyLoss\nimport numpy as np\nfrom sat.model.mixins import CachedAutoregressiveMixin\nfrom sat.generation.autoregressive_sampling import filling_sequence\nfrom sat.generation.sampling_strategies import BaseStrategy, BeamSearchStrategy\ndef chat(model, tokenizer, tokens,\n         max_length: int = 1800, num_beams=5, top_p=0.95, top_k=0, temperature=0.8, **kwargs):\n    inputs = tokens.to(model.parameters().__next__().device)[0]",
        "type": "code",
        "location": "/finetune_demo/finetune_cogagent_demo.py:110-140"
    },
    "307": {
        "file_id": 16,
        "content": "109-139: Function to get batch data, handles broadcasting and potential type conversions based on args.\nget_batch: Loads data, broadcasts it if data_iterator is not None, checks data type for possible fp16 or bf16 conversion, returns data batch.",
        "type": "comment"
    },
    "308": {
        "file_id": 16,
        "content": "    seq = torch.cat(\n        [inputs, torch.tensor([-1] * (max_length - len(inputs)), device=inputs.device)], dim=0\n    )\n    strategy = BaseStrategy(temperature=temperature, top_p=0.4, top_k=1, end_tokens=[tokenizer.eos_token_id])\n    # strategy = BeamSearchStrategy(temperature=temperature, top_p=top_p, top_k=top_k, end_tokens=[tokenizer.eos_token_id],\n    #                               num_beams=num_beams, consider_end=True)\n    get_func = llama2_text_processor_inference.get_func(None, None, image_rope_mask=kwargs['image_rope_mask'])\n    output = filling_sequence(\n        model, seq,\n        batch_size=1,\n        strategy=strategy,\n        get_masks_and_position_ids=get_func,\n        **kwargs\n    )[0]  # drop memory\n    return output\ndef forward_step_eval(data_iterator, model, args, timers):\n    def compute_metrics(eval_preds):\n        preds, labels, device = eval_preds\n        preds = preds.unsqueeze(0)\n        if isinstance(preds, tuple):\n            preds = preds[0]\n        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)",
        "type": "code",
        "location": "/finetune_demo/finetune_cogagent_demo.py:141-165"
    },
    "309": {
        "file_id": 16,
        "content": "The code is concatenating input tokens with a padding token (-1) to fill up the maximum sequence length. It then applies a BaseStrategy or BeamSearchStrategy (depending on parameters) for decoding the generated text. The get_func is used to retrieve masks and position IDs for the given text prompt. Finally, forward_step_eval defines a compute_metrics function that decodes the predicted sequences and compares them with the ground truth labels.",
        "type": "comment"
    },
    "310": {
        "file_id": 16,
        "content": "        if args.ignore_pad_token_for_loss:\n            # Replace -100 in the labels as we can't decode them.\n            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n        score_dict = {\n            \"acc\": [],\n            \"acc_w/o_case\": [],\n        }\n        for pred, label in zip(decoded_preds, decoded_labels):\n            if args.rank == 0:\n                print('pred', pred, 'label', label, flush=True)\n            if pred == label:\n                score_dict['acc'].append(1.)\n            else:\n                score_dict['acc'].append(0.)\n            if pred.lower() == label.lower():\n                score_dict['acc_w/o_case'].append(1.)\n            else:\n                score_dict['acc_w/o_case'].append(0.)\n        for k, v in score_dict.items():\n            score_dict[k] = float(np.mean(v))\n        return score_dict\n    # Get the batch.\n    timers('batch generator').start()\n    data_b = get_batch(\n        data_iterator, args, timers)",
        "type": "code",
        "location": "/finetune_demo/finetune_cogagent_demo.py:166-195"
    },
    "311": {
        "file_id": 16,
        "content": "This code calculates accuracy metrics for a model's predictions and labels. It replaces -100 in the labels with the pad token id, decodes the labels and predictions using the tokenizer, then compares them to calculate the accuracy and accuracy without considering case. Finally, it returns the calculated metrics as a dictionary.",
        "type": "comment"
    },
    "312": {
        "file_id": 16,
        "content": "    timers('batch generator').stop()\n    context_len = int(data_b['context_length'][0])\n    tokens = data_b['input_ids'][:, :context_len]\n    data_b['vision_expert_mask'] = data_b['vision_expert_mask'][:, :context_len]\n    data_b['image_embed_mask'] = data_b['image_embed_mask'][:, :context_len]\n    data_b['image_rope_mask'] = data_b['image_rope_mask'][:, :context_len]\n    data_b.pop('input_ids')\n    data_b.pop('attention_mask')\n    data_b.pop('position_ids')\n    labels = data_b.pop('labels')\n    qid = data_b.pop('question_id')\n    model.add_mixin('auto-regressive', CachedAutoregressiveMixin())\n    outputs = chat(model, tokenizer, tokens, **data_b)[0][context_len:]\n    # print(outputs)\n    model.del_mixin('auto-regressive')\n    return torch.tensor(0, device=outputs.device), {k: torch.tensor(v, device=outputs.device) for k, v in\n                                                    compute_metrics(\n                                                        (outputs.cpu(), labels.cpu(), outputs.device)).items()}",
        "type": "code",
        "location": "/finetune_demo/finetune_cogagent_demo.py:196-217"
    },
    "313": {
        "file_id": 16,
        "content": "The code is preparing the data and fine-tuning a model for a specific context length, then generating outputs using an autoregressive mixin. It also computes metrics for evaluation and returns results.",
        "type": "comment"
    },
    "314": {
        "file_id": 16,
        "content": "from torch.nn import CrossEntropyLoss\ndef forward_step(data_iterator, model, args, timers):\n    \"\"\"Forward step.\"\"\"\n    # Get the batch.\n    timers('batch generator').start()\n    data_b = get_batch(\n        data_iterator, args, timers)\n    labels = data_b.pop('labels')\n    timers('batch generator').stop()\n    logits = model(**data_b)[0]\n    lm_logits = logits.to(torch.float32)\n    # Shift so that tokens < n predict n\n    shift_labels = labels[..., 1:].contiguous()\n    shift_logits = lm_logits[..., -1-shift_labels.size(-1):-1, :].contiguous()\n    # Flatten the tokens\n    loss_fct = CrossEntropyLoss(ignore_index=-100)\n    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n    loss = loss.to(torch.float32)\n    return loss, {'loss': loss}\nfrom utils.utils import ItemDataset\ndef create_dataset_function(image_processor, text_processor, cross_image_processor, path, args):\n    dataset = ItemDataset(image_processor, text_processor, args, path, cross_image_processor=cross_image_processor)",
        "type": "code",
        "location": "/finetune_demo/finetune_cogagent_demo.py:220-244"
    },
    "315": {
        "file_id": 16,
        "content": "This code defines two functions. The first function, `forward_step`, performs a forward step in a neural network model. It takes input data from a data iterator, processes it, and calculates the loss between predicted logits and actual labels. The second function, `create_dataset_function`, creates a dataset by combining image and text processors with specified arguments and paths.",
        "type": "comment"
    },
    "316": {
        "file_id": 16,
        "content": "    return dataset\nfrom sat.model.finetune.lora2 import LoraMixin\nfrom sat.model.finetune.prompt_tuning import PTuningV2Mixin\nif __name__ == '__main__':\n    py_parser = argparse.ArgumentParser(add_help=False)\n    py_parser.add_argument('--max_length', type=int)\n    py_parser.add_argument('--ignore_pad_token_for_loss', action='store_false')\n    py_parser.add_argument(\"--version\", type=str, default=\"chat\", choices=[\"chat\", \"vqa\"], help='version to interact with')\n    py_parser.add_argument(\"--from_pretrained\", type=str, default=\"cogagent-chat\", help='pretrained ckpt')\n    py_parser.add_argument(\"--local_tokenizer\", type=str, default=\"lmsys/vicuna-7b-v1.5\", help='tokenizer path')\n    py_parser.add_argument(\"--vit_checkpoint_activations\", action='store_true')\n    py_parser = FineTuneTrainCogAgentModel.add_model_specific_args(py_parser)\n    known, args_list = py_parser.parse_known_args()\n    args = get_args(args_list)\n    args = argparse.Namespace(**vars(args), **vars(known))\n    if args.use_qlora:\n        args.device = 'cpu'",
        "type": "code",
        "location": "/finetune_demo/finetune_cogagent_demo.py:245-263"
    },
    "317": {
        "file_id": 16,
        "content": "This code defines a function that returns a dataset and uses the argparse module to parse command-line arguments for fine-tuning a CogAgent model. It also imports classes LoraMixin and PTuningV2Mixin from sat.model.finetune, and sets default values for various model-specific arguments. If args.use_qlora is True, the device will be set to 'cpu'.",
        "type": "comment"
    },
    "318": {
        "file_id": 16,
        "content": "    model, args = FineTuneTrainCogAgentModel.from_pretrained(args.from_pretrained, args, overwrite_args={'model_parallel_size': args.model_parallel_size} if args.model_parallel_size != 1 else {})\n    if args.use_ptuning: # TODO: wait for SAT updating\n        model.add_mixin(\"ptuning\", PTuningV2Mixin(args.num_layers, args.hidden_size // args.num_attention_heads, args.num_attention_heads, args.pre_seq_len))\n    if args.use_lora:\n        model.add_mixin(\"lora\", LoraMixin(args.num_layers, args.lora_rank, layer_range=args.layer_range), reinit=True)\n        model.get_mixin(\"eva\").vit_model.add_mixin(\"lora\", LoraMixin(args.eva_args['num_layers'], args.lora_rank, layer_range=args.layer_range), reinit=True)\n    elif args.use_qlora:\n        model.add_mixin(\"lora\", LoraMixin(args.num_layers, args.lora_rank, layer_range=args.layer_range, qlora=True), reinit=True)\n    if args.use_qlora and torch.cuda.is_available():\n        model = model.to('cuda')\n    from utils.utils import llama2_tokenizer\n    tokenizer = llama2_tokenizer(args.local_tokenizer, signal_type=args.version)",
        "type": "code",
        "location": "/finetune_demo/finetune_cogagent_demo.py:265-278"
    },
    "319": {
        "file_id": 16,
        "content": "This code initializes a fine-tuning model for the CogAgent using the specified pretrained model and arguments. It adds mixins such as PTuning, Lora, or QLora based on the provided options. The model is then sent to GPU if available. Finally, it creates a tokenizer instance from the specified local tokenizer and signal type.",
        "type": "comment"
    },
    "320": {
        "file_id": 16,
        "content": "    image_processor = get_image_processor(args.eva_args[\"image_size\"][0])\n    cross_image_processor = get_image_processor(args.cross_image_pix)\n    text_processor = llama2_text_processor(tokenizer, args.max_length, args.image_length)\n    model = training_main(args, model_cls=model, forward_step_function=forward_step, create_dataset_function=partial(create_dataset_function, image_processor, text_processor, cross_image_processor), collate_fn=partial(data_collator, cross_image_processor=cross_image_processor), forward_step_eval=forward_step_eval)\n    if args.use_lora:\n        model.get_mixin(\"lora\").merge_lora()\n        model.get_mixin(\"eva\").vit_model.get_mixin(\"lora\").merge_lora()\n        args.use_lora = False\n        args.save = \"checkpoints/merged_lora_cogagent\"\n        from sat.training.model_io import save_checkpoint\n        save_checkpoint(1, model, None, None, args)",
        "type": "code",
        "location": "/finetune_demo/finetune_cogagent_demo.py:279-290"
    },
    "321": {
        "file_id": 16,
        "content": "Initializing image, cross-image, and text processors; creating model using training main function with provided arguments; merging Lora if enabled; saving merged model.",
        "type": "comment"
    },
    "322": {
        "file_id": 17,
        "content": "/finetune_demo/finetune_cogagent_lora.sh",
        "type": "filepath"
    },
    "323": {
        "file_id": 17,
        "content": "The script sets environment variables, specifies model details, and runs a language model finetuning process using Deepspeed, NCCL, and CUDA with distributed backend, cosine learning rate decay, checkpoint activations, and saves/evaluates every 200 steps.",
        "type": "summary"
    },
    "324": {
        "file_id": 17,
        "content": "#! /bin/bash\n# export PATH=/usr/local/cuda/bin:$PATH\n# export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH\nNUM_GPUS_PER_WORKER=8\nMP_SIZE=1\nscript_path=$(realpath $0)\nscript_dir=$(dirname $script_path)\nmain_dir=$(dirname $script_dir)\nMODEL_TYPE=\"cogagent-chat\"\nVERSION=\"chat\"\nMODEL_ARGS=\"--from_pretrained $MODEL_TYPE \\\n    --max_length 400 \\\n    --lora_rank 50 \\\n    --use_lora \\\n    --local_tokenizer lmsys/vicuna-7b-v1.5 \\\n    --version $VERSION\"\n# TIPS: max_length include low-resolution image sequence (which has 256 tokens) \nOPTIONS_SAT=\"SAT_HOME=~/.sat_models\"\nOPTIONS_NCCL=\"NCCL_DEBUG=info NCCL_IB_DISABLE=0 NCCL_NET_GDR_LEVEL=2 LOCAL_WORLD_SIZE=$NUM_GPUS_PER_WORKER\"\nHOST_FILE_PATH=\"hostfile\"\ntrain_data=\"./archive_split/train\"\nvalid_data=\"./archive_split/valid\"\ngpt_options=\" \\\n       --experiment-name finetune-$MODEL_TYPE \\\n       --model-parallel-size ${MP_SIZE} \\\n       --mode finetune \\\n       --train-iters 2000 \\\n       --resume-dataloader \\\n       $MODEL_ARGS \\\n       --train-data ${train_data} \\\n       --valid-data ${valid_data} \\",
        "type": "code",
        "location": "/finetune_demo/finetune_cogagent_lora.sh:1-36"
    },
    "325": {
        "file_id": 17,
        "content": "This script is setting environment variables for CUDA and LD_LIBRARY_PATH, defining the number of GPUs per worker and MPI size. It also specifies the model type, version, arguments, options for SAT and NCCL, path to host file, and data locations for training and validation.",
        "type": "comment"
    },
    "326": {
        "file_id": 17,
        "content": "       --distributed-backend nccl \\\n       --lr-decay-style cosine \\\n       --warmup .02 \\\n       --checkpoint-activations \\\n       --vit_checkpoint_activations \\\n       --save-interval 200 \\\n       --eval-interval 200 \\\n       --save \"./checkpoints\" \\\n       --eval-iters 10 \\\n       --eval-batch-size 1 \\\n       --split 1. \\\n       --deepspeed_config test_config_bf16.json \\\n       --skip-init \\\n       --seed 2023\n\"\nrun_cmd=\"${OPTIONS_NCCL} ${OPTIONS_SAT} deepspeed --master_port 16666 --hostfile ${HOST_FILE_PATH} finetune_cogagent_demo.py ${gpt_options}\"\necho ${run_cmd}\neval ${run_cmd}\nset +x",
        "type": "code",
        "location": "/finetune_demo/finetune_cogagent_lora.sh:37-59"
    },
    "327": {
        "file_id": 17,
        "content": "This script is finetuning a language model using Deepspeed, a distributed training framework. It uses NCCL for distributed backend, cosine learning rate decay style, 0.2 warmup ratio, checkpoint activations and VIT checkpoint activations, saves checkpoints every 200 steps, evaluates the model every 200 steps, and runs on a machine with hostfile located at ${HOST_FILE_PATH}. The master port is 16666. It uses a test configuration file named \"test_config_bf16.json\", skips initialization, sets a seed value of 2023, and executes the finetune_cogagent_demo.py script with provided GPT options.",
        "type": "comment"
    },
    "328": {
        "file_id": 18,
        "content": "/finetune_demo/finetune_cogvlm_demo.py",
        "type": "filepath"
    },
    "329": {
        "file_id": 18,
        "content": "The code imports modules for training CogVLM models, processes data and tensor elements, generates chat responses using a given model, adjusts parameters, utilizes language model, performs evaluation, manages command line arguments, handles LORA/QLORA application, moves model to GPU if available, initializes processors, and saves merged model.",
        "type": "summary"
    },
    "330": {
        "file_id": 18,
        "content": "import os\nimport torch\nimport argparse\nfrom functools import partial\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom sat import mpu, get_args, get_tokenizer\nfrom sat.training.deepspeed_training import training_main\nfrom sat.helpers import print_rank0\nfrom utils.models import FineTuneTrainCogVLMModel\nfrom utils.utils import llama2_text_processor, llama2_text_processor_inference, get_image_processor\ndef disable_untrainable_params(self):\n    total_trainable = 0\n    enable = [('mlp', 'vit')]\n    if self.args.use_ptuning:\n        enable.extend(['ptuning'])\n    if self.args.use_lora or self.args.use_qlora:\n        enable.extend(['matrix_A', 'matrix_B'])\n    for n, p in self.named_parameters():\n        flag = False\n        for e in enable:\n            if type(e) is tuple:\n                if e[0].lower() in n.lower() and e[1].lower() in n.lower() and 55 > int(n[:n.find('.mlp')].split('.')[-1]) > 45:\n                    flag = True\n                    break\n            else:\n                if e.lower() in n.lower():",
        "type": "code",
        "location": "/finetune_demo/finetune_cogvlm_demo.py:1-29"
    },
    "331": {
        "file_id": 18,
        "content": "This code is importing necessary modules, defining a function to disable untrainable parameters in a model, and setting up enable flags for specific types of parameter tuning. It appears to be part of a larger fine-tuning process for a CogVLM model, potentially using DeepSpeed training.",
        "type": "comment"
    },
    "332": {
        "file_id": 18,
        "content": "                    flag = True\n                    break\n        if not flag:\n            p.requires_grad_(False)\n        else:\n            total_trainable += p.numel()\n            print_rank0(n)\n    print_rank0(\"***** Total trainable parameters: \"+str(total_trainable)+\" *****\")\nFineTuneTrainCogVLMModel.disable_untrainable_params = disable_untrainable_params\ndef data_collator(examples):\n    examples = [ex for ex in examples if len(ex) > 0] # drop {}\n    for example in examples:\n        for k in example:\n            if isinstance(example[k], list):\n                example[k] = torch.tensor(example[k])\n            elif isinstance(example[k], np.ndarray):\n                example[k] = torch.from_numpy(example[k])\n    img_args = {}\n    tmp_example = examples[0]\n    for k in tmp_example['vision']:\n        if type(tmp_example['vision'][k]) is torch.Tensor:\n            img_args['vision_'+k] = torch.cat([example['vision'][k] for example in examples])\n        else:\n            img_args['vision_'+k] = example['vision'][k]",
        "type": "code",
        "location": "/finetune_demo/finetune_cogvlm_demo.py:30-55"
    },
    "333": {
        "file_id": 18,
        "content": "This code is defining a function `data_collator` that processes examples and creates a dictionary of tensors for training. It checks if the list is empty, converts lists to tensors, converts numpy arrays to tensors, and concatenates images into one tensor.",
        "type": "comment"
    },
    "334": {
        "file_id": 18,
        "content": "    for example in examples:\n        example.pop('vision')\n        if 'cross' in example:\n            example.pop('cross')\n    model_args = {}\n    tmp_example = examples[0]\n    for k in tmp_example:\n        if type(tmp_example[k]) is torch.Tensor:\n            model_args[k] = torch.cat([example[k] for example in examples])\n        else:\n            model_args[k] = tmp_example[k]\n    model_args.update(img_args)\n    return model_args\nfrom collections import defaultdict\ndef broadcast_auto(data_dict):\n    type2list = defaultdict(list)\n    other = []\n    for k in data_dict:\n        if type(data_dict[k]) is torch.Tensor:\n            type2list[data_dict[k].dtype].append(k)\n        else:\n            other.append(k)\n    new_data = {}\n    for k in type2list:\n        new_data.update(mpu.broadcast_data(type2list[k], data_dict, k))\n    for k in other:\n        new_data[k] = data_dict[k]\n    return new_data\ndef get_batch(data_iterator, args, timers):\n    # Broadcast data.\n    timers('data loader').start()\n    if data_iterator is not None:",
        "type": "code",
        "location": "/finetune_demo/finetune_cogvlm_demo.py:56-91"
    },
    "335": {
        "file_id": 18,
        "content": "This code is processing a dictionary of data, separating tensor and non-tensor elements, then broadcasting the tensor data. It returns a new dictionary with the processed data. The function \"get_batch\" retrieves data from an iterator and applies this processing.",
        "type": "comment"
    },
    "336": {
        "file_id": 18,
        "content": "        data = next(data_iterator)\n    else:\n        data = None\n    timers('data loader').stop()\n    data_b = broadcast_auto(data)\n    for k in data_b:\n        if type(data_b[k]) is torch.Tensor and data_b[k].dtype is not torch.int32 and data_b[k].dtype is not torch.long:\n            if args.fp16:\n                data_b[k] = data_b[k].half()\n            elif args.bf16:\n                data_b[k] = data_b[k].bfloat16()\n    return data_b\nfrom torch.nn import CrossEntropyLoss\nimport numpy as np\nfrom sat.model.mixins import CachedAutoregressiveMixin\nfrom sat.generation.autoregressive_sampling import filling_sequence\nfrom sat.generation.sampling_strategies import BaseStrategy, BeamSearchStrategy\ndef chat(model, tokenizer, tokens,\n         max_length: int = 1800, num_beams=5, top_p=0.95, top_k=0, temperature=0.8, **kwargs):\n    inputs = tokens.to(model.parameters().__next__().device)[0]\n    seq = torch.cat(\n        [inputs, torch.tensor([-1] * (max_length - len(inputs)), device=inputs.device)], dim=0\n    )\n    str",
        "type": "code",
        "location": "/finetune_demo/finetune_cogvlm_demo.py:92-119"
    },
    "337": {
        "file_id": 18,
        "content": "Function: chat\nDescription: Generates a response from the given model, tokenizer, tokens.\nInputs: model (transformer model), tokenizer (tokenizing function), tokens (input tokens), max_length (maximum length of output), num_beams (number of beams for beam search), top_p (sampling probability), top_k (top k values for sampling).\nOutputs: Response generated by the model.",
        "type": "comment"
    },
    "338": {
        "file_id": 18,
        "content": "ategy = BaseStrategy(temperature=temperature, top_p=0.4, top_k=1, end_tokens=[tokenizer.eos_token_id])\n    # strategy = BeamSearchStrategy(temperature=temperature, top_p=top_p, top_k=top_k, end_tokens=[tokenizer.eos_token_id],\n    #                               num_beams=num_beams, consider_end=True)\n    get_func = llama2_text_processor_inference.get_func(None, None, image_rope_mask=kwargs['image_rope_mask'])\n    output = filling_sequence(\n        model, seq,\n        batch_size=1,\n        strategy=strategy,\n        get_masks_and_position_ids=get_func,\n        **kwargs\n    )[0]  # drop memory\n    return output\ndef forward_step_eval(data_iterator, model, args, timers):\n    def compute_metrics(eval_preds):\n        preds, labels, device = eval_preds\n        preds = preds.unsqueeze(0)\n        if isinstance(preds, tuple):\n            preds = preds[0]\n        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n        if args.ignore_pad_token_for_loss:\n            # Replace -100 in the labels as we can't decode them.",
        "type": "code",
        "location": "/finetune_demo/finetune_cogvlm_demo.py:119-142"
    },
    "339": {
        "file_id": 18,
        "content": "Strategy is set based on input parameters.\nFunction \"filling_sequence\" called with model, sequence, batch size, strategy, and additional kwargs.",
        "type": "comment"
    },
    "340": {
        "file_id": 18,
        "content": "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n        score_dict = {\n            \"acc\": [],\n            \"acc_w/o_case\": [],\n        }\n        for pred, label in zip(decoded_preds, decoded_labels):\n            if args.rank == 0:\n                print('pred', pred, 'label', label, flush=True)\n            if pred == label:\n                score_dict['acc'].append(1.)\n            else:\n                score_dict['acc'].append(0.)\n            if pred.lower() == label.lower():\n                score_dict['acc_w/o_case'].append(1.)\n            else:\n                score_dict['acc_w/o_case'].append(0.)\n        for k, v in score_dict.items():\n            score_dict[k] = float(np.mean(v))\n        return score_dict\n    # Get the batch.\n    timers('batch generator').start()\n    data_b = get_batch(\n        data_iterator, args, timers)\n    timers('batch generator').stop()\n    context_len = int(data_b['context_length'][0])",
        "type": "code",
        "location": "/finetune_demo/finetune_cogvlm_demo.py:143-173"
    },
    "341": {
        "file_id": 18,
        "content": "This code calculates accuracy and accuracy without case sensitivity for the predictions and labels. It first converts the labels into padded token IDs or keeps them as the pad_token if they are -100, then decodes the label and prediction batches. The code then iterates over the pairs of decoded predictions and labels, checking if they match or not for both case-sensitive and case-insensitive scenarios. It stores the counts in a dictionary and calculates the average to get the accuracy scores. After that, it returns the score_dict containing the averaged accuracy and accuracy without case sensitivity. The code also handles a batch generator and gets a batch of data.",
        "type": "comment"
    },
    "342": {
        "file_id": 18,
        "content": "    tokens = data_b['input_ids'][:, :context_len]\n    data_b['vision_expert_mask'] = data_b['vision_expert_mask'][:, :context_len]\n    data_b['image_embed_mask'] = data_b['image_embed_mask'][:, :context_len]\n    data_b['image_rope_mask'] = data_b['image_rope_mask'][:, :context_len]\n    data_b.pop('input_ids')\n    data_b.pop('attention_mask')\n    data_b.pop('position_ids')\n    labels = data_b.pop('labels')\n    qid = data_b.pop('question_id')\n    model.add_mixin('auto-regressive', CachedAutoregressiveMixin())\n    outputs = chat(model, tokenizer, tokens, **data_b)[0][context_len:]\n    # print(outputs)\n    model.del_mixin('auto-regressive')\n    return torch.tensor(0, device=outputs.device), {k: torch.tensor(v, device=outputs.device) for k, v in\n                                                    compute_metrics(\n                                                        (outputs.cpu(), labels.cpu(), outputs.device)).items()}\nfrom torch.nn import CrossEntropyLoss\ndef forward_step(data_iterator, model, args, timers):",
        "type": "code",
        "location": "/finetune_demo/finetune_cogvlm_demo.py:174-196"
    },
    "343": {
        "file_id": 18,
        "content": "This code chunk is finetuning a model for a specific task using the CogVLM model. It extracts relevant data from a dataset, applies necessary transformations to adapt it to the model's requirements, and passes it through the model to generate predictions. The function returns a tensor of 0 and a dictionary containing computed metrics for evaluation. The code also includes calls to a `chat` function, CachedAutoregressiveMixin(), and compute_metrics().",
        "type": "comment"
    },
    "344": {
        "file_id": 18,
        "content": "    \"\"\"Forward step.\"\"\"\n    # Get the batch.\n    timers('batch generator').start()\n    data_b = get_batch(\n        data_iterator, args, timers)\n    labels = data_b.pop('labels')\n    timers('batch generator').stop()\n    logits = model(**data_b)[0]\n    lm_logits = logits.to(torch.float32)\n    # Shift so that tokens < n predict n\n    shift_labels = labels[..., 1:].contiguous()\n    shift_logits = lm_logits[..., -1-shift_labels.size(-1):-1, :].contiguous()\n    # Flatten the tokens\n    loss_fct = CrossEntropyLoss(ignore_index=-100)\n    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n    loss = loss.to(torch.float32)\n    return loss, {'loss': loss}\nfrom utils.utils import ItemDataset\ndef create_dataset_function(image_processor, text_processor, path, args):\n    dataset = ItemDataset(image_processor, text_processor, args, path)\n    return dataset\nfrom sat.model.finetune.lora2 import LoraMixin\nfrom sat.model.finetune.prompt_tuning import PTuningV2Mixin\nif __name__ == '__main__':\n    py_parser = argparse.ArgumentParser(add_help=False)",
        "type": "code",
        "location": "/finetune_demo/finetune_cogvlm_demo.py:197-226"
    },
    "345": {
        "file_id": 18,
        "content": "Code is performing forward pass for a language model fine-tuning demonstration. It gets the batch data, passes it through the model to get logits, shifts logits and labels accordingly, calculates loss using CrossEntropyLoss, returns loss and other information. The function create_dataset_function creates a dataset function by combining image and text processors and arguments. Additionally, there's an if condition for checking if the script is being run as main.",
        "type": "comment"
    },
    "346": {
        "file_id": 18,
        "content": "    py_parser.add_argument('--max_length', type=int)\n    py_parser.add_argument('--ignore_pad_token_for_loss', action='store_false')\n    py_parser.add_argument(\"--version\", type=str, default=\"chat_old\", help='version to interact with')\n    py_parser.add_argument(\"--from_pretrained\", type=str, default=\"cogvlm-chat\", help='pretrained ckpt')\n    py_parser.add_argument(\"--local_tokenizer\", type=str, default=\"lmsys/vicuna-7b-v1.5\", help='tokenizer path')\n    py_parser.add_argument(\"--vit_checkpoint_activations\", action='store_true')\n    py_parser = FineTuneTrainCogVLMModel.add_model_specific_args(py_parser)\n    known, args_list = py_parser.parse_known_args()\n    args = get_args(args_list)\n    args = argparse.Namespace(**vars(args), **vars(known))\n    if args.use_qlora:\n        args.device = 'cpu'\n    model, args = FineTuneTrainCogVLMModel.from_pretrained(args.from_pretrained, args, overwrite_args={'model_parallel_size': args.model_parallel_size} if args.model_parallel_size != 1 else {})\n    if args.use_ptuning:",
        "type": "code",
        "location": "/finetune_demo/finetune_cogvlm_demo.py:227-241"
    },
    "347": {
        "file_id": 18,
        "content": "This code is parsing command line arguments for training a CogVLM model. The arguments specify the maximum sequence length, whether to ignore padding tokens in loss calculation, the version of the model to interact with, pre-trained checkpoint, tokenizer path, activations from vision transformer (ViT) checkpoints, and device settings. The code also sets up the model on the appropriate device based on whether the user is using QLORA or P-tuning.",
        "type": "comment"
    },
    "348": {
        "file_id": 18,
        "content": "        model.add_mixin(\"ptuning\", PTuningV2Mixin(args.num_layers, args.hidden_size // args.num_attention_heads, args.num_attention_heads, args.pre_seq_len))\n    if args.use_lora:\n        model.add_mixin(\"lora\", LoraMixin(args.num_layers, args.lora_rank, layer_range=args.layer_range), reinit=True)\n        model.get_mixin(\"eva\").vit_model.add_mixin(\"lora\", LoraMixin(args.eva_args['num_layers'], args.lora_rank, layer_range=args.layer_range), reinit=True)\n    elif args.use_qlora:\n        model.add_mixin(\"lora\", LoraMixin(args.num_layers, args.lora_rank, layer_range=args.layer_range, qlora=True), reinit=True)\n    if args.use_qlora and torch.cuda.is_available():\n        model = model.to('cuda')\n    from utils.utils import llama2_tokenizer\n    tokenizer = llama2_tokenizer(args.local_tokenizer, signal_type=args.version)\n    image_processor = get_image_processor(args.eva_args[\"image_size\"][0])\n    text_processor = llama2_text_processor(tokenizer, args.max_length, args.image_length)\n    model = training_ma",
        "type": "code",
        "location": "/finetune_demo/finetune_cogvlm_demo.py:242-256"
    },
    "349": {
        "file_id": 18,
        "content": "The code adds mixins to the model, applies LORA or QLORA if specified, moves the model to GPU if available, loads a tokenizer, and initializes text and image processors for training.",
        "type": "comment"
    },
    "350": {
        "file_id": 18,
        "content": "in(args, model_cls=model, forward_step_function=forward_step, create_dataset_function=partial(create_dataset_function, image_processor, text_processor), collate_fn=data_collator, forward_step_eval=forward_step_eval)\n    if args.use_lora:\n        model.get_mixin(\"lora\").merge_lora()\n        model.get_mixin(\"eva\").vit_model.get_mixin(\"lora\").merge_lora()\n        args.use_lora = False\n        args.save = \"checkpoints/merged_lora_cogvlm{}\".format(args.eva_args[\"image_size\"][0])\n        from sat.training.model_io import save_checkpoint\n        save_checkpoint(1, model, None, None, args)",
        "type": "code",
        "location": "/finetune_demo/finetune_cogvlm_demo.py:256-263"
    },
    "351": {
        "file_id": 18,
        "content": "Applies LORA and saves merged model",
        "type": "comment"
    },
    "352": {
        "file_id": 19,
        "content": "/finetune_demo/finetune_cogvlm_lora.sh",
        "type": "filepath"
    },
    "353": {
        "file_id": 19,
        "content": "This script sets up environment and options for fine-tuning a CogVLM language model using LORA and model parallelism, training for 800 iterations on provided datasets. The DeepSpeed command is used to save checkpoints, perform evaluations, and use a master port of 16666 for communication.",
        "type": "summary"
    },
    "354": {
        "file_id": 19,
        "content": "#! /bin/bash\n# export PATH=/usr/local/cuda/bin:$PATH\n# export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH\nNUM_GPUS_PER_WORKER=8\nMP_SIZE=1\nscript_path=$(realpath $0)\nscript_dir=$(dirname $script_path)\nmain_dir=$(dirname $script_dir)\nMODEL_TYPE=\"cogvlm-base-490\"\nVERSION=\"base\"\nMODEL_ARGS=\"--from_pretrained $MODEL_TYPE \\\n    --max_length 1288 \\\n    --lora_rank 10 \\\n    --use_lora \\\n    --local_tokenizer lmsys/vicuna-7b-v1.5 \\\n    --version $VERSION\"\n# Tips: If training models of resolution 244, you can set --max_length smaller \nOPTIONS_SAT=\"SAT_HOME=~/.sat_models\"\nOPTIONS_NCCL=\"NCCL_DEBUG=info NCCL_IB_DISABLE=0 NCCL_NET_GDR_LEVEL=2 LOCAL_WORLD_SIZE=$NUM_GPUS_PER_WORKER\"\nHOST_FILE_PATH=\"hostfile\"\ntrain_data=\"./archive_split/train\"\nvalid_data=\"./archive_split/valid\"\ngpt_options=\" \\\n       --experiment-name finetune-$MODEL_TYPE \\\n       --model-parallel-size ${MP_SIZE} \\\n       --mode finetune \\\n       --train-iters 800 \\\n       --resume-dataloader \\\n       $MODEL_ARGS \\\n       --train-data ${train_data} \\\n       --valid-data ${valid_data} \\",
        "type": "code",
        "location": "/finetune_demo/finetune_cogvlm_lora.sh:1-36"
    },
    "355": {
        "file_id": 19,
        "content": "This script is for finetuning a CogVLM language model using LORA and model parallelism. It sets up the necessary environment variables, arguments, and options for training and data loading. The model will be trained for 800 iterations on provided train and validation datasets.",
        "type": "comment"
    },
    "356": {
        "file_id": 19,
        "content": "       --distributed-backend nccl \\\n       --lr-decay-style cosine \\\n       --warmup .02 \\\n       --checkpoint-activations \\\n       --vit_checkpoint_activations \\\n       --save-interval 200 \\\n       --eval-interval 200 \\\n       --save \"./checkpoints\" \\\n       --eval-iters 10 \\\n       --eval-batch-size 1 \\\n       --split 1. \\\n       --deepspeed_config test_config_bf16.json \\\n       --skip-init \\\n       --seed 2023\n\"\nrun_cmd=\"${OPTIONS_NCCL} ${OPTIONS_SAT} deepspeed --master_port 16666 --hostfile ${HOST_FILE_PATH} finetune_cogvlm_demo.py ${gpt_options}\"\necho ${run_cmd}\neval ${run_cmd}\nset +x",
        "type": "code",
        "location": "/finetune_demo/finetune_cogvlm_lora.sh:37-59"
    },
    "357": {
        "file_id": 19,
        "content": "The code is setting various options and running a DeepSpeed command to fine-tune the CogVLM model using specified options. It saves checkpoints at regular intervals, performs evaluations every 200 iterations, and uses a master port of 16666 for communication.",
        "type": "comment"
    },
    "358": {
        "file_id": 20,
        "content": "/finetune_demo/test_config_bf16.json",
        "type": "filepath"
    },
    "359": {
        "file_id": 20,
        "content": "The code configures a training model with micro batch size, gradient accumulation steps, and BF16 enabled. It also includes gradient clipping, zero optimization settings, offload optimizer on the CPU with pin memory, activation checkpointing options, and disables wall clock breakdown tracking.",
        "type": "summary"
    },
    "360": {
        "file_id": 20,
        "content": "{\n    \"train_micro_batch_size_per_gpu\": 4,\n    \"gradient_accumulation_steps\": 1,\n    \"gradient_clipping\": 0.1,\n    \"zero_optimization\": {\n      \"stage\": 2,\n      \"contiguous_gradients\": false,\n      \"overlap_comm\": true,\n      \"reduce_scatter\": true,\n      \"reduce_bucket_size\": 4e7,\n      \"allgather_bucket_size\": 1e8,\n      \"load_from_fp32_weights\": false\n    },\n    \"offload_optimizer\": {\n      \"device\": \"cpu\",\n      \"pin_memory\": true\n    },\n    \"zero_allow_untested_optimizer\": true,\n    \"bf16\": {\n      \"enabled\": true\n    },\n    \"optimizer\": {\n      \"type\": \"Adam\",\n      \"params\": {\n        \"lr\": 0.00001,\n        \"betas\": [\n          0.9,\n          0.95\n        ],\n        \"eps\": 1e-8,\n        \"weight_decay\": 5e-2\n      }\n    },\n    \"activation_checkpointing\": {\n      \"partition_activations\": false,\n      \"contiguous_memory_optimization\": false,\n      \"cpu_checkpointing\": false\n    },\n    \"wall_clock_breakdown\": false\n  }",
        "type": "code",
        "location": "/finetune_demo/test_config_bf16.json:1-40"
    },
    "361": {
        "file_id": 20,
        "content": "The code configures a training model with micro batch size, gradient accumulation steps, and BF16 enabled. It also includes gradient clipping, zero optimization settings, offload optimizer on the CPU with pin memory, activation checkpointing options, and disables wall clock breakdown tracking.",
        "type": "comment"
    },
    "362": {
        "file_id": 21,
        "content": "/openai_demo/openai_api.py",
        "type": "filepath"
    },
    "363": {
        "file_id": 21,
        "content": "The code creates a FastAPI app for chat functionality, with endpoints for message interactions, model listings, and advanced model-based conversation generation. It also handles GPU memory management, CORS, and checks device presence before starting the server using Uvicorn.",
        "type": "summary"
    },
    "364": {
        "file_id": 21,
        "content": "import os\nimport gc\nimport time\nimport base64\nfrom contextlib import asynccontextmanager\nfrom typing import List, Literal, Union, Tuple, Optional\nimport torch\nimport uvicorn\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom loguru import logger\nfrom pydantic import BaseModel, Field\nfrom sse_starlette.sse import EventSourceResponse\nfrom transformers import AutoModelForCausalLM, LlamaTokenizer, PreTrainedModel, PreTrainedTokenizer, \\\n    TextIteratorStreamer\nfrom PIL import Image\nfrom io import BytesIO\nMODEL_PATH = os.environ.get('MODEL_PATH', 'THUDM/cogvlm-chat-hf')\nTOKENIZER_PATH = os.environ.get(\"TOKENIZER_PATH\", 'lmsys/vicuna-7b-v1.5')\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nif os.environ.get('QUANT_ENABLED'):\n    QUANT_ENABLED = True\nelse:\n    with torch.cuda.device(DEVICE):\n        __, total_bytes = torch.cuda.mem_get_info()\n        total_gb = total_bytes / (1 << 30)\n        if total_gb < 40:\n            QUANT_ENABLED = True\n        else:\n            QUANT_ENABLED = False",
        "type": "code",
        "location": "/openai_demo/openai_api.py:1-32"
    },
    "365": {
        "file_id": 21,
        "content": "Importing various libraries and setting up configurations for the model.",
        "type": "comment"
    },
    "366": {
        "file_id": 21,
        "content": "@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"\n    An asynchronous context manager for managing the lifecycle of the FastAPI app.\n    It ensures that GPU memory is cleared after the app's lifecycle ends, which is essential for efficient resource management in GPU environments.\n    \"\"\"\n    yield\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.ipc_collect()\napp = FastAPI(lifespan=lifespan)\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\nclass ModelCard(BaseModel):\n    \"\"\"\n    A Pydantic model representing a model card, which provides metadata about a machine learning model.\n    It includes fields like model ID, owner, and creation time.\n    \"\"\"\n    id: str\n    object: str = \"model\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    owned_by: str = \"owner\"\n    root: Optional[str] = None\n    parent: Optional[str] = None\n    permission: Optional[list] = None",
        "type": "code",
        "location": "/openai_demo/openai_api.py:34-68"
    },
    "367": {
        "file_id": 21,
        "content": "This code defines a class `ModelCard` using Pydantic, which is used to represent metadata about a machine learning model. It includes fields like model ID, owner, and creation time. The `lifespan` function ensures that GPU memory is cleared after the app's lifecycle ends in environments with GPUs available. The FastAPI app also uses a CORSMiddleware for handling Cross-Origin Resource Sharing, allowing requests from any origin (\"*\"), with credentials (\"true\"), for all methods (\"*\") and headers (\"*\").",
        "type": "comment"
    },
    "368": {
        "file_id": 21,
        "content": "class ModelList(BaseModel):\n    object: str = \"list\"\n    data: List[ModelCard] = []\nclass ImageUrl(BaseModel):\n    url: str\nclass TextContent(BaseModel):\n    type: Literal[\"text\"]\n    text: str\nclass ImageUrlContent(BaseModel):\n    type: Literal[\"image_url\"]\n    image_url: ImageUrl\nContentItem = Union[TextContent, ImageUrlContent]\nclass ChatMessageInput(BaseModel):\n    role: Literal[\"user\", \"assistant\", \"system\"]\n    content: Union[str, List[ContentItem]]\n    name: Optional[str] = None\nclass ChatMessageResponse(BaseModel):\n    role: Literal[\"assistant\"]\n    content: str = None\n    name: Optional[str] = None\nclass DeltaMessage(BaseModel):\n    role: Optional[Literal[\"user\", \"assistant\", \"system\"]] = None\n    content: Optional[str] = None\nclass ChatCompletionRequest(BaseModel):\n    model: str\n    messages: List[ChatMessageInput]\n    temperature: Optional[float] = 0.8\n    top_p: Optional[float] = 0.8\n    max_tokens: Optional[int] = None\n    stream: Optional[bool] = False\n    # Additional parameters\n    repetition_penalty: Optional[float] = 1.0",
        "type": "code",
        "location": "/openai_demo/openai_api.py:71-118"
    },
    "369": {
        "file_id": 21,
        "content": "This code defines various classes and types for handling chat messages, model lists, image URLs, text content, and more. These are used to interact with OpenAI's API for generating chat responses. The `ChatMessageInput` represents a message input, while the `ChatCompletionRequest` is used to specify parameters for generating chat responses using the specified model.",
        "type": "comment"
    },
    "370": {
        "file_id": 21,
        "content": "class ChatCompletionResponseChoice(BaseModel):\n    index: int\n    message: ChatMessageResponse\nclass ChatCompletionResponseStreamChoice(BaseModel):\n    index: int\n    delta: DeltaMessage\nclass UsageInfo(BaseModel):\n    prompt_tokens: int = 0\n    total_tokens: int = 0\n    completion_tokens: Optional[int] = 0\nclass ChatCompletionResponse(BaseModel):\n    model: str\n    object: Literal[\"chat.completion\", \"chat.completion.chunk\"]\n    choices: List[Union[ChatCompletionResponseChoice, ChatCompletionResponseStreamChoice]]\n    created: Optional[int] = Field(default_factory=lambda: int(time.time()))\n    usage: Optional[UsageInfo] = None\n@app.get(\"/v1/models\", response_model=ModelList)\nasync def list_models():\n    \"\"\"\n    An endpoint to list available models. It returns a list of model cards.\n    This is useful for clients to query and understand what models are available for use.\n    \"\"\"\n    model_card = ModelCard(id=\"cogvlm-chat-17b\")  # can be replaced by your model id like cogagent-chat-18b\n    return ModelList(data=[model_card])",
        "type": "code",
        "location": "/openai_demo/openai_api.py:121-152"
    },
    "371": {
        "file_id": 21,
        "content": "Class definitions for ChatCompletionResponseChoice, ChatCompletionResponseStreamChoice, UsageInfo and ChatCompletionResponse.\nAn API endpoint to list available models with model cards information.",
        "type": "comment"
    },
    "372": {
        "file_id": 21,
        "content": "@app.post(\"/v1/chat/completions\", response_model=ChatCompletionResponse)\nasync def create_chat_completion(request: ChatCompletionRequest):\n    global model, tokenizer\n    if len(request.messages) < 1 or request.messages[-1].role == \"assistant\":\n        raise HTTPException(status_code=400, detail=\"Invalid request\")\n    gen_params = dict(\n        messages=request.messages,\n        temperature=request.temperature,\n        top_p=request.top_p,\n        max_tokens=request.max_tokens or 1024,\n        echo=False,\n        stream=request.stream,\n    )\n    if request.stream:\n        generate = predict(request.model, gen_params)\n        return EventSourceResponse(generate, media_type=\"text/event-stream\")\n    response = generate_cogvlm(model, tokenizer, gen_params)\n    usage = UsageInfo()\n    message = ChatMessageResponse(\n        role=\"assistant\",\n        content=response[\"text\"],\n    )\n    logger.debug(f\"==== message ====\\n{message}\")\n    choice_data = ChatCompletionResponseChoice(\n        index=0,\n        message=message,",
        "type": "code",
        "location": "/openai_demo/openai_api.py:155-185"
    },
    "373": {
        "file_id": 21,
        "content": "This code defines a POST route for creating chat completions. It checks the validity of the input, then generates a response based on the provided parameters and model. If the stream parameter is set, it returns an EventSourceResponse to stream the generation progress. Otherwise, it calls generate_cogvlm function to generate the response. The usage information is recorded and the generated message is logged for debugging purposes.",
        "type": "comment"
    },
    "374": {
        "file_id": 21,
        "content": "    )\n    task_usage = UsageInfo.model_validate(response[\"usage\"])\n    for usage_key, usage_value in task_usage.model_dump().items():\n        setattr(usage, usage_key, getattr(usage, usage_key) + usage_value)\n    return ChatCompletionResponse(model=request.model, choices=[choice_data], object=\"chat.completion\", usage=usage)\nasync def predict(model_id: str, params: dict):\n    \"\"\"\n    Handle streaming predictions. It continuously generates responses for a given input stream.\n    This is particularly useful for real-time, continuous interactions with the model.\n    \"\"\"\n    global model, tokenizer\n    choice_data = ChatCompletionResponseStreamChoice(\n        index=0,\n        delta=DeltaMessage(role=\"assistant\"),\n        finish_reason=None\n    )\n    chunk = ChatCompletionResponse(model=model_id, choices=[choice_data], object=\"chat.completion.chunk\")\n    yield \"{}\".format(chunk.model_dump_json(exclude_unset=True))\n    previous_text = \"\"\n    for new_response in generate_stream_cogvlm(model, tokenizer, params):\n        decoded_unicode = new_response[\"text\"]",
        "type": "code",
        "location": "/openai_demo/openai_api.py:186-211"
    },
    "375": {
        "file_id": 21,
        "content": "This code is handling streaming predictions for real-time, continuous interactions with the model. It generates responses based on input stream and continuously yields these responses as JSON objects.",
        "type": "comment"
    },
    "376": {
        "file_id": 21,
        "content": "        delta_text = decoded_unicode[len(previous_text):]\n        previous_text = decoded_unicode\n        delta = DeltaMessage(\n            content=delta_text,\n            role=\"assistant\",\n        )\n        choice_data = ChatCompletionResponseStreamChoice(\n            index=0,\n            delta=delta,\n        )\n        chunk = ChatCompletionResponse(model=model_id, choices=[choice_data], object=\"chat.completion.chunk\")\n        yield \"{}\".format(chunk.model_dump_json(exclude_unset=True))\n    choice_data = ChatCompletionResponseStreamChoice(\n        index=0,\n        delta=DeltaMessage(),\n    )\n    chunk = ChatCompletionResponse(model=model_id, choices=[choice_data], object=\"chat.completion.chunk\")\n    yield \"{}\".format(chunk.model_dump_json(exclude_unset=True))\ndef generate_cogvlm(model: PreTrainedModel, tokenizer: PreTrainedTokenizer, params: dict):\n    \"\"\"\n    Generates a response using the CogVLM model. It processes the chat history and image data, if any,\n    and then invokes the model to generate a response.",
        "type": "code",
        "location": "/openai_demo/openai_api.py:212-235"
    },
    "377": {
        "file_id": 21,
        "content": "This code creates a response using the CogVLM model and processes chat history, handling assistant role and chunking response.",
        "type": "comment"
    },
    "378": {
        "file_id": 21,
        "content": "    \"\"\"\n    for response in generate_stream_cogvlm(model, tokenizer, params):\n        pass\n    return response\ndef process_history_and_images(messages: List[ChatMessageInput]) -> Tuple[\n    Optional[str], Optional[List[Tuple[str, str]]], Optional[List[Image.Image]]]:\n    \"\"\"\n    Process history messages to extract text, identify the last user query,\n    and convert base64 encoded image URLs to PIL images.\n    Args:\n        messages(List[ChatMessageInput]): List of ChatMessageInput objects.\n    return: A tuple of three elements:\n             - The last user query as a string.\n             - Text history formatted as a list of tuples for the model.\n             - List of PIL Image objects extracted from the messages.\n    \"\"\"\n    formatted_history = []\n    image_list = []\n    last_user_query = ''\n    for i, message in enumerate(messages):\n        role = message.role\n        content = message.content\n        if isinstance(content, list):  # text\n            text_content = ' '.join(item.text for item in content if isinstance(item, TextContent))",
        "type": "code",
        "location": "/openai_demo/openai_api.py:236-265"
    },
    "379": {
        "file_id": 21,
        "content": "Processing messages to extract text and images for ChatGPT-like API.",
        "type": "comment"
    },
    "380": {
        "file_id": 21,
        "content": "        else:\n            text_content = content\n        if isinstance(content, list):  # image\n            for item in content:\n                if isinstance(item, ImageUrlContent):\n                    image_url = item.image_url.url\n                    if image_url.startswith(\"data:image/jpeg;base64,\"):\n                        base64_encoded_image = image_url.split(\"data:image/jpeg;base64,\")[1]\n                        image_data = base64.b64decode(base64_encoded_image)\n                        image = Image.open(BytesIO(image_data)).convert('RGB')\n                        image_list.append(image)\n        if role == 'user':\n            if i == len(messages) - 1:  # \n                last_user_query = text_content\n            else:\n                formatted_history.append((text_content, ''))\n        elif role == 'assistant':\n            if formatted_history:\n                if formatted_history[-1][1] != '':\n                    assert False, f\"the last query is answered. answer again. {formatted_history[-1][0]}, {formatted_history[-1][1]}, {text_content}\"",
        "type": "code",
        "location": "/openai_demo/openai_api.py:266-287"
    },
    "381": {
        "file_id": 21,
        "content": "This code handles various types of content and extracts necessary information based on the role (user or assistant) and the position within a message history. For user messages, it checks if it's the last message in the history and assigns the text to 'last_user_query'. For assistant responses, it ensures that the previous question has been answered before generating a new response.",
        "type": "comment"
    },
    "382": {
        "file_id": 21,
        "content": "                formatted_history[-1] = (formatted_history[-1][0], text_content)\n            else:\n                assert False, f\"assistant reply before user\"\n        else:\n            assert False, f\"unrecognized role: {role}\"\n    return last_user_query, formatted_history, image_list\n@torch.inference_mode()\ndef generate_stream_cogvlm(model: PreTrainedModel, tokenizer: PreTrainedTokenizer, params: dict):\n    \"\"\"\n    Generates a stream of responses using the CogVLM model in inference mode.\n    It's optimized to handle continuous input-output interactions with the model in a streaming manner.\n    \"\"\"\n    messages = params[\"messages\"]\n    temperature = float(params.get(\"temperature\", 1.0))\n    repetition_penalty = float(params.get(\"repetition_penalty\", 1.0))\n    top_p = float(params.get(\"top_p\", 1.0))\n    max_new_tokens = int(params.get(\"max_tokens\", 256))\n    query, history, image_list = process_history_and_images(messages)\n    logger.debug(f\"==== request ====\\n{query}\")\n    input_by_model = model.build_conversation_input_ids(tokenizer, query=query, history=history,",
        "type": "code",
        "location": "/openai_demo/openai_api.py:288-312"
    },
    "383": {
        "file_id": 21,
        "content": "The code is part of a function that generates a stream of responses using the CogVLM model. It takes in messages, temperature, repetition penalty, top p, and maximum number of tokens as input parameters. It processes history and images from the messages and passes them to the model for conversation generation.",
        "type": "comment"
    },
    "384": {
        "file_id": 21,
        "content": "                                                        images=[image_list[-1]])\n    inputs = {\n        'input_ids': input_by_model['input_ids'].unsqueeze(0).to(DEVICE),\n        'token_type_ids': input_by_model['token_type_ids'].unsqueeze(0).to(DEVICE),\n        'attention_mask': input_by_model['attention_mask'].unsqueeze(0).to(DEVICE),\n        'images': [[input_by_model['images'][0].to(DEVICE).to(torch_type)]],\n    }\n    if 'cross_images' in input_by_model and input_by_model['cross_images']:\n        inputs['cross_images'] = [[input_by_model['cross_images'][0].to(DEVICE).to(torch_type)]]\n    input_echo_len = len(inputs[\"input_ids\"][0])\n    streamer = TextIteratorStreamer(\n        tokenizer=tokenizer,\n        timeout=60.0,\n        skip_prompt=True,\n        skip_special_tokens=True\n)\n    gen_kwargs = {\n        \"repetition_penalty\": repetition_penalty,\n        \"max_new_tokens\": max_new_tokens,\n        \"do_sample\": True if temperature > 1e-5 else False,\n        \"top_p\": top_p if temperature > 1e-5 else 0,\n        'streamer': streamer,",
        "type": "code",
        "location": "/openai_demo/openai_api.py:313-335"
    },
    "385": {
        "file_id": 21,
        "content": "This code prepares input data for a language model and sets up the streamer for generating text. The input includes 'input_ids', 'token_type_ids', and 'attention_mask' for the text, and optionally 'cross_images'. It also defines the maximum number of tokens to generate ('max_new_tokens'), whether to sample or generate deterministically based on temperature, and sets up the TextIteratorStreamer for generating text.",
        "type": "comment"
    },
    "386": {
        "file_id": 21,
        "content": "    }\n    if temperature > 1e-5:\n        gen_kwargs[\"temperature\"] = temperature\n    total_len = 0\n    generated_text = \"\"\n    with torch.no_grad():\n        model.generate(**inputs, **gen_kwargs)\n        for next_text in streamer:\n            generated_text += next_text\n            yield {\n                \"text\": generated_text,\n                \"usage\": {\n                    \"prompt_tokens\": input_echo_len,\n                    \"completion_tokens\": total_len - input_echo_len,\n                    \"total_tokens\": total_len,\n                },\n            }\n    ret = {\n        \"text\": generated_text,\n        \"usage\": {\n            \"prompt_tokens\": input_echo_len,\n            \"completion_tokens\": total_len - input_echo_len,\n            \"total_tokens\": total_len,\n        },\n    }\n    yield ret\ngc.collect()\ntorch.cuda.empty_cache()\nif __name__ == \"__main__\":\n    tokenizer = LlamaTokenizer.from_pretrained(\n        TOKENIZER_PATH,\n        trust_remote_code=True)\n    if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8:",
        "type": "code",
        "location": "/openai_demo/openai_api.py:336-373"
    },
    "387": {
        "file_id": 21,
        "content": "In this code, we have a function that generates text using an AI model. It checks the temperature value and updates it if necessary. The generated text is accumulated and returned in chunks along with usage information (prompt_tokens, completion_tokens, total_tokens). After generating the text, it cleans up memory by calling gc.collect() and torch.cuda.empty_cache(). If the CUDA capability of the GPU is version 8 or higher, it initializes the tokenizer from a specific pretrained model.",
        "type": "comment"
    },
    "388": {
        "file_id": 21,
        "content": "        torch_type = torch.bfloat16\n    else:\n        torch_type = torch.float16\n    print(\"========Use torch type as:{} with device:{}========\\n\\n\".format(torch_type, DEVICE))\n    if 'cuda' in DEVICE:\n        if QUANT_ENABLED:\n            model = AutoModelForCausalLM.from_pretrained(\n                MODEL_PATH,\n                load_in_4bit=True,\n                trust_remote_code=True,\n                torch_dtype=torch_type,\n                low_cpu_mem_usage=True\n            ).eval()\n        else:\n            model = AutoModelForCausalLM.from_pretrained(\n                MODEL_PATH,\n                load_in_4bit=False,\n                trust_remote_code=True,\n                torch_dtype=torch_type,\n                low_cpu_mem_usage=True\n            ).to(DEVICE).eval()\n    else:\n        model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, trust_remote_code=True).float().to(DEVICE).eval()\n    uvicorn.run(app, host='0.0.0.0', port=8000, workers=1)",
        "type": "code",
        "location": "/openai_demo/openai_api.py:374-400"
    },
    "389": {
        "file_id": 21,
        "content": "Checking if CUDA device is present, sets torch type to bfloat16 or float16 depending on quantization enabled. Instantiates AutoModelForCausalLM model based on settings and moves it to the specified device (either CPU or CUDA). Finally, starts a server for the app using Uvicorn.",
        "type": "comment"
    },
    "390": {
        "file_id": 22,
        "content": "/openai_demo/openai_api_request.py",
        "type": "filepath"
    },
    "391": {
        "file_id": 22,
        "content": "The script introduces an OpenAI API simulator for CogVLM and CogAgent Chat, facilitating image and text input via chat function. It initializes a chatbot convo with an image description and a user question, using the OpenAI API to generate a completion based on the given input.",
        "type": "summary"
    },
    "392": {
        "file_id": 22,
        "content": "\"\"\"\nThis script is designed to mimic the OpenAI API interface with CogVLM & CogAgent Chat\nIt demonstrates how to integrate image and text-based input to generate a response.\nCurrently, the model can only handle a single image.\nTherefore, do not use this script to process multiple images in one conversation. (includes images from history)\nAnd it only works on the chat model, not the base model.\n\"\"\"\nimport requests\nimport json\nimport base64\nbase_url = \"http://127.0.0.1:8000\"\ndef create_chat_completion(model, messages, temperature=0.8, max_tokens=2048, top_p=0.8, use_stream=False):\n    \"\"\"\n    This function sends a request to the chat API to generate a response based on the given messages.\n    Args:\n        model (str): The name of the model to use for generating the response.\n        messages (list): A list of message dictionaries representing the conversation history.\n        temperature (float): Controls randomness in response generation. Higher values lead to more random responses.\n        max_tokens (int): The maximum length of the generated response.",
        "type": "code",
        "location": "/openai_demo/openai_api_request.py:1-23"
    },
    "393": {
        "file_id": 22,
        "content": "This script mimics the OpenAI API for CogVLM and CogAgent Chat, integrating image and text input to generate a response. It currently supports single images with the chat model only. The create_chat_completion function sends requests to the chat API based on conversation history.",
        "type": "comment"
    },
    "394": {
        "file_id": 22,
        "content": "        top_p (float): Controls diversity of response by filtering less likely options.\n        use_stream (bool): Determines whether to use a streaming response or a single response.\n    The function constructs a JSON payload with the specified parameters and sends a POST request to the API.\n    It then handles the response, either as a stream (for ongoing responses) or a single message.\n    \"\"\"\n    data = {\n        \"model\": model,\n        \"messages\": messages,\n        \"stream\": use_stream,\n        \"max_tokens\": max_tokens,\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n    }\n    response = requests.post(f\"{base_url}/v1/chat/completions\", json=data, stream=use_stream)\n    if response.status_code == 200:\n        if use_stream:\n            # \n            for line in response.iter_lines():\n                if line:\n                    decoded_line = line.decode('utf-8')[6:]\n                    try:\n                        response_json = json.loads(decoded_line)\n                        content = response_json.get(\"choices\", [{}])[0].get(\"delta\", {}).get(\"content\", \"\")",
        "type": "code",
        "location": "/openai_demo/openai_api_request.py:24-49"
    },
    "395": {
        "file_id": 22,
        "content": "This code defines a function that sends an OpenAI API request for chat completion using specified parameters. It constructs a JSON payload with model, messages, stream (whether to use streaming response), max_tokens, temperature, and top_p (diversity control). It then sends a POST request to the API and handles the response as either a single message or a streaming response.",
        "type": "comment"
    },
    "396": {
        "file_id": 22,
        "content": "                        print(content)\n                    except:\n                        print(\"Special Token:\", decoded_line)\n        else:\n            # \n            decoded_line = response.json()\n            content = decoded_line.get(\"choices\", [{}])[0].get(\"message\", \"\").get(\"content\", \"\")\n            print(content)\n    else:\n        print(\"Error:\", response.status_code)\n        return None\ndef encode_image(image_path):\n    \"\"\"\n    Encodes an image file into a base64 string.\n    Args:\n        image_path (str): The path to the image file.\n    This function opens the specified image file, reads its content, and encodes it into a base64 string.\n    The base64 encoding is used to send images over HTTP as text.\n    \"\"\"\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode(\"utf-8\")\ndef simple_image_chat(use_stream=True, img_path=None):\n    \"\"\"\n    Facilitates a simple chat interaction involving an image.\n    Args:\n        use_stream (bool): Specifies whether to use streaming for chat responses.",
        "type": "code",
        "location": "/openai_demo/openai_api_request.py:50-82"
    },
    "397": {
        "file_id": 22,
        "content": "Processing API request for OpenAI text completion.\nHandling non-streaming chat responses from OpenAI.\nEncoding an image file into a base64 string using the encode_image function.",
        "type": "comment"
    },
    "398": {
        "file_id": 22,
        "content": "        img_path (str): Path to the image file to be included in the chat.\n    This function encodes the specified image and constructs a predefined conversation involving the image.\n    It then calls `create_chat_completion` to generate a response from the model.\n    The conversation includes asking about the content of the image and a follow-up question.\n    \"\"\"\n    img_url = f\"data:image/jpeg;base64,{encode_image(img_path)}\"\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\", \"text\": \"Whats in this image?\",\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": img_url\n                    },\n                },\n            ],\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"The image displays a wooden boardwalk extending through a vibrant green grassy wetland. The sky is partly cloudy with soft, wispy clouds,",
        "type": "code",
        "location": "/openai_demo/openai_api_request.py:83-108"
    },
    "399": {
        "file_id": 22,
        "content": "This function encodes an image and creates a chat conversation around it, then uses OpenAI's API to generate a response from the model. It includes questions about the image content.",
        "type": "comment"
    }
}