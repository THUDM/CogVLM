# CogVLM & CogAgent

ğŸ“— [README in English](./README.md)

ğŸŒŸ **è·³è½¬åˆ°è¯¦ç»†ä»‹ç»: [CogVLMä»‹ç»](#introduction-to-cogvlm)ï¼Œ
ğŸ†• [CogAgentçš„ä»‹ç»](#introduction-to-cogagent)**

ğŸ“” å¦‚éœ€è·å–æ›´è¯¦ç»†çš„ä½¿ç”¨ä¿¡æ¯ï¼Œè¯·å‚é˜…: [CogVLM&CogAgentæŠ€æœ¯æ–‡æ¡£](https://zhipu-ai.feishu.cn/wiki/LXQIwqo1OiIVTykMh9Lc3w1Fn7g)

<table>
  <tr>
    <td>
      <h2> CogVLM </h2>
      <p> ğŸ“–  Paper: <a href="https://arxiv.org/abs/2311.03079">CogVLM: Visual Expert for Pretrained Language Models</a></p>
      <p><b>CogVLM</b> æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å¼€æºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€‚CogVLM-17Bæ‹¥æœ‰100äº¿çš„è§†è§‰å‚æ•°å’Œ70äº¿çš„è¯­è¨€å‚æ•°ï¼Œæ”¯æŒ490*490åˆ†è¾¨ç‡çš„å›¾åƒç†è§£å’Œå¤šè½®å¯¹è¯ã€‚</p>
      <p><b>CogVLM-17B 17Båœ¨10ä¸ªç»å…¸çš„è·¨æ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½</b>åŒ…æ‹¬NoCaps, Flicker30k captioning, RefCOCO, RefCOCO+, RefCOCOg, Visual7W, GQA, ScienceQA, VizWiz VQA å’Œ TDIUC åŸºå‡†æµ‹è¯•ã€‚</p>
    </td>
    <td>
      <h2> CogAgent </h2>
      <p> ğŸ“–  Paper: <a href="https://arxiv.org/abs/2312.08914">CogAgent: A Visual Language Model for GUI Agents </a></p>
      <p><b>CogAgent</b> æ˜¯ä¸€ä¸ªåŸºäºCogVLMæ”¹è¿›çš„å¼€æºè§†è§‰è¯­è¨€æ¨¡å‹ã€‚CogAgent-18Bæ‹¥æœ‰110äº¿çš„è§†è§‰å‚æ•°å’Œ70äº¿çš„è¯­è¨€å‚æ•°, <b>æ”¯æŒ1120*1120åˆ†è¾¨ç‡çš„å›¾åƒç†è§£ã€‚åœ¨CogVLMçš„èƒ½åŠ›ä¹‹ä¸Šï¼Œå®ƒè¿›ä¸€æ­¥æ‹¥æœ‰äº†GUIå›¾åƒAgentçš„èƒ½åŠ›ã€‚</b></p>
      <p> <b>CogAgent-18B åœ¨9ä¸ªç»å…¸çš„è·¨æ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„é€šç”¨æ€§èƒ½ï¼Œ</b>åŒ…æ‹¬ VQAv2, OK-VQ, TextVQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, å’Œ POPE æµ‹è¯•åŸºå‡†ã€‚å®ƒåœ¨åŒ…æ‹¬AITWå’ŒMind2Webåœ¨å†…çš„GUIæ“ä½œæ•°æ®é›†ä¸Šæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„æ¨¡å‹ã€‚</p>
    </td>
  </tr>
  <tr>
    <td colspan="2" align="center">
      <p>ğŸŒ CogVLM å’Œ CogAgent çš„ç½‘ç»œæ¼”ç¤º: <a href="http://36.103.203.44:7861">this link</a></p>
    </td>
  </tr>
</table>


**ç›®å½•**

- [CogVLM \& CogAgent](#cogvlm--cogagent)
    - [Release](#å‘å¸ƒ)
    - [å¼€å§‹ä½¿ç”¨](#å¼€å§‹ä½¿ç”¨)
        - [é€‰é¡¹1ï¼šä½¿ç”¨ç½‘é¡µæ¼”ç¤ºè¿›è¡Œæ¨ç†](#é€‰é¡¹1ä½¿ç”¨ç½‘é¡µæ¼”ç¤ºè¿›è¡Œæ¨ç†)
        - [é€‰é¡¹2ï¼šè‡ªè¡Œéƒ¨ç½²CogVLM / CogAgent](#é€‰é¡¹2è‡ªè¡Œéƒ¨ç½²cogvlm--cogagent)
            - [Situation 2.1 CLI (SAT version)](#situation-21-cli-sat-version)
            - [Situation 2.2 CLI (Huggingface version)](#situation-22-cli-huggingface-version)
            - [Situation 2.3 Web Demo](#situation-23-web-demo)
        - [é€‰é¡¹3ï¼šå¾®è°ƒ CogAgent / CogVLM](#é€‰é¡¹3å¾®è°ƒ-cogagent--cogvlm)
        - [é€‰é¡¹4ï¼šOpenAIæ ¼å¼](#é€‰é¡¹4OpenAIæ ¼å¼)
        - [ç¡¬ä»¶éœ€æ±‚](#ç¡¬ä»¶éœ€æ±‚)
        - [Model checkpoints](#model-checkpoints)
    - [Introduction to CogVLM](#introduction-to-cogvlm)
        - [ç¤ºä¾‹](#ç¤ºä¾‹)
    - [Introduction to CogAgent](#introduction-to-cogagent)
        - [GUI Agent Examples](#gui-agent-examples)
    - [Cookbook](#cookbook)
        - [Task Prompts](#task-prompts)
        - [é€‰æ‹©é€‚åˆçš„æ¨¡å‹](#é€‰æ‹©é€‚åˆçš„æ¨¡å‹)
    - [License](#license)
    - [Citation \& Acknowledgements](#citation--acknowledgements)

## å‘å¸ƒ
- ğŸ”¥ğŸ”¥ğŸ”¥  **News**: ```2024/4/5```: [CogAgent](https://arxiv.org/abs/2312.08914) æˆåŠŸè¢«è¯„é€‰ä¸ºCVPR 2024 Highlights!
- ğŸ”¥ğŸ”¥ **News**: ```2023/12/26```:æˆ‘ä»¬å…¬å¼€äº† [CogVLM-SFT-311K](dataset_zh.md) æ•°æ®é›†ï¼Œå®ƒåŒ…å«äº†è¶…è¿‡15ä¸‡æ¡æˆ‘ä»¬ç”¨äºè®­ç»ƒ **CogVLM v1.0(ä»…è¯¥æ¨¡å‹)** çš„æ•°æ®ã€‚æ¬¢è¿å…³æ³¨å’Œä½¿ç”¨ã€‚
- ğŸ”¥ **News**: ```2023/12/18```: **æ–°çš„Streamlitç”¨æˆ·ç•Œé¢**å·²ç»ä¸Šçº¿ï¼æˆ‘ä»¬å·²ç»åŸºäºStreamlitæ¨å‡ºäº†æ–°çš„ç½‘é¡µç”¨æˆ·ç•Œé¢ï¼Œç”¨æˆ·å¯ä»¥åœ¨æˆ‘ä»¬çš„ç•Œé¢ä¸Šè½»æ¾ä¸CogVLMï¼ŒCogAgentäº¤è°ˆã€‚å¸¦æ¥æ›´å¥½çš„ç”¨æˆ·ä½“éªŒã€‚
- ğŸ”¥ **News**: ```2023/12/15```: **CogAgent æ­£å¼å‘å¸ƒï¼** CogAgentæ˜¯åŸºäºCogVLMå¼€å‘çš„å›¾åƒç†è§£æ¨¡å‹ã€‚å®ƒå…·æœ‰åŸºäºè§†è§‰çš„GUI
  AgentåŠŸèƒ½ï¼Œå¹¶åœ¨å›¾åƒç†è§£æ–¹é¢è¿›è¡Œäº†è¿›ä¸€æ­¥çš„å¢å¼ºã€‚å®ƒæ”¯æŒåˆ†è¾¨ç‡ä¸º1120*1120çš„å›¾åƒè¾“å…¥ï¼Œå¹¶å…·æœ‰åŒ…æ‹¬ä¸å›¾åƒè¿›è¡Œå¤šè½®å¯¹è¯ã€GUI
  Agentã€Groundingç­‰å¤šç§èƒ½åŠ›ã€‚

- **News**: ```2023/12/8```:
  æˆ‘ä»¬å·²å°†cogvlm-grounding-generalistçš„æ£€æŸ¥ç‚¹æ›´æ–°ä¸ºcogvlm-grounding-generalist-v1.1ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­å¢åŠ äº†å›¾åƒå¢å¼ºï¼Œå› æ­¤æ›´åŠ ç¨³å¥ã€‚æŸ¥çœ‹[è¯¦æƒ…](#introduction-to-cogvlm)ã€‚

- **News**: ```2023/12/7``` CogVLMç°åœ¨æ”¯æŒ**4-bit**é‡åŒ–ï¼æ‚¨åªéœ€è¦11GBçš„GPUå†…å­˜å°±å¯ä»¥è¿›è¡Œæ¨ç†ï¼

- **News**: ```2023/11/20```æˆ‘ä»¬å·²å°†cogvlm-chatçš„æ£€æŸ¥ç‚¹æ›´æ–°ä¸ºcogvlm-chat-v1.1ï¼Œç»Ÿä¸€äº†èŠå¤©å’ŒVQAçš„ç‰ˆæœ¬ï¼Œå¹¶åˆ·æ–°äº†å„ç§æ•°æ®é›†ä¸Šçš„SOTAï¼ŒæŸ¥çœ‹[è¯¦æƒ…](#introduction-to-cogvlm)ã€‚

- **News**: ```2023/11/20``` æˆ‘ä»¬åœ¨ğŸ¤—Huggingfaceä¸Šå‘å¸ƒäº† **[cogvlm-chat](https://huggingface.co/THUDM/cogvlm-chat-hf)**, **[cogvlm-grounding-generalist](https://huggingface.co/THUDM/cogvlm-grounding-generalist-hf)/[base](https://huggingface.co/THUDM/cogvlm-grounding-base-hf)**, **[cogvlm-base-490](https://huggingface.co/THUDM/cogvlm-base-490-hf)/[224](https://huggingface.co/THUDM/cogvlm-base-224-hf)**ï¼Œä½¿ç”¨transformers å¿«é€Ÿ [æ¨ç†](#situation-22-cli-huggingface-version)ã€‚

- ```2023/10/27``` CogVLMåŒè¯­ç‰ˆæœ¬å·²ç»åœ¨çº¿ä¸Šå¯ç”¨ï¼æ¬¢è¿[è¯•ç”¨](https://chatglm.cn/)ã€‚

- ```2023/10/5``` CogVLM-17B v1.0 å‘å¸ƒã€‚

## å¼€å§‹ä½¿ç”¨

### é€‰é¡¹1ï¼šä½¿ç”¨ç½‘é¡µæ¼”ç¤ºè¿›è¡Œæ¨ç†

* ç‚¹å‡»æ­¤å¤„è¿›å…¥ [CogVLM & CogAgent Web Demo](http://36.103.203.44:7861/)ã€‚

å¦‚æœæ‚¨éœ€è¦ä½¿ç”¨ä»£ç†å’Œæ¥åœ°åŠŸèƒ½ï¼Œè¯·å‚è€ƒ[Cookbook - Task Prompts](#task-prompts)ã€‚

### é€‰é¡¹2ï¼šè‡ªè¡Œéƒ¨ç½²CogVLM / CogAgent

æˆ‘ä»¬æ”¯æŒä¸¤ç§æ¨¡å‹æ¨ç†çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼Œå‘½ä»¤è¡Œç•Œé¢å’Œç½‘ç»œæ¼”ç¤ºã€‚å¦‚æœä½ æƒ³åœ¨ä½ çš„Pythonä»£ç ä¸­ä½¿ç”¨å®ƒï¼Œä¿®æ”¹å‘½ä»¤è¡Œè„šæœ¬ä»¥é€‚åº”ä½ çš„æƒ…å†µã€‚
é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å®‰è£…ä¾èµ–é¡¹ã€‚

```bash
# CUDA >= 11.8
pip install -r requirements.txt
python -m spacy download en_core_web_sm
```

**æ‰€æœ‰çš„æ¨ç†ä»£ç éƒ½ä½äº `basic_demo/` ç›®å½•ä¸‹ã€‚è¯·åœ¨è¿›è¡Œè¿›ä¸€æ­¥æ“ä½œä¹‹å‰ï¼Œå…ˆåˆ‡æ¢åˆ°è¿™ä¸ªç›®å½•ã€‚**

#### Situation 2.1 CLI (SAT version)

é€šè¿‡ä»¥ä¸‹æ–¹å¼è¿è¡ŒCLIæ¼”ç¤ºï¼š

```bash
# CogAgent
python cli_demo_sat.py --from_pretrained cogagent-chat --version chat --bf16  --stream_chat
python cli_demo_sat.py --from_pretrained cogagent-vqa --version chat_old --bf16  --stream_chat

# CogVLM
python cli_demo_sat.py --from_pretrained cogvlm-chat --version chat_old --bf16  --stream_chat
python cli_demo_sat.py --from_pretrained cogvlm-grounding-generalist --version base --bf16  --stream_chat
```

è¯¥ç¨‹åºå°†è‡ªåŠ¨ä¸‹è½½å«æ˜Ÿæ¨¡å‹å¹¶åœ¨å‘½ä»¤è¡Œä¸­è¿›è¡Œäº¤äº’ã€‚æ‚¨å¯ä»¥é€šè¿‡è¾“å…¥æŒ‡ä»¤å¹¶æŒ‰å›è½¦æ¥ç”Ÿæˆå›å¤ã€‚è¾“å…¥`clear` ä»¥æ¸…é™¤å¯¹è¯å†å²ï¼Œè¾“å…¥`stop` ä»¥åœæ­¢ç¨‹åºã€‚

æˆ‘ä»¬ä¹Ÿæ”¯æŒæ¨¡å‹å¹¶è¡Œæ¨ç†ï¼Œè¯¥æ¨ç†å°†æ¨¡å‹åˆ†å‰²åˆ°å¤šä¸ªï¼ˆ2/4/8ï¼‰GPUä¸Šã€‚ä½¿ç”¨ `--nproc-per-node=[n]` æ§åˆ¶ä½¿ç”¨çš„GPUæ•°é‡ã€‚

```
torchrun --standalone --nnodes=1 --nproc-per-node=2 cli_demo_sat.py --from_pretrained cogagent-chat --version chat --bf16
```

- å¦‚æœä½ æƒ³æ‰‹åŠ¨ä¸‹è½½æƒé‡ï¼Œä½ å¯ä»¥ç”¨æ¨¡å‹è·¯å¾„æ›¿æ¢ ``--from_pretrained`` åçš„è·¯å¾„ã€‚

- æˆ‘ä»¬çš„æ¨¡å‹æ”¯æŒSATçš„4ä½é‡åŒ–å’Œ8ä½é‡åŒ–ã€‚ä½ å¯ä»¥å°† ``--bf16`` æ›´æ”¹ä¸º ``--fp16``, æˆ– ``--fp16 --quant 4``, æˆ– ``--fp16 --quant 8``.

  ä¾‹å¦‚

    ```bash
    python cli_demo_sat.py --from_pretrained cogagent-chat --fp16 --quant 8 --stream_chat
    python cli_demo_sat.py --from_pretrained cogvlm-chat-v1.1 --fp16 --quant 4 --stream_chat
    # In SAT versionï¼Œ--quant should be used with --fp16
    ```

- è¯¥ç¨‹åºæä¾›ä»¥ä¸‹è¶…å‚æ•°æ¥æ§åˆ¶ç”Ÿæˆè¿‡ç¨‹ï¼š
    ```
    usage: cli_demo_sat.py [-h] [--max_length MAX_LENGTH] [--top_p TOP_P] [--top_k TOP_K] [--temperature TEMPERATURE]

    optional arguments:
    -h, --help            show this help message and exit
    --max_length MAX_LENGTH
                            max length of the total sequence
    --top_p TOP_P         top p for nucleus sampling
    --top_k TOP_K         top k for top k sampling
    --temperature TEMPERATURE
                            temperature for sampling
    ```

- ç‚¹å‡» [è¿™é‡Œ](#which---version-to-use) æŸ¥çœ‹ä¸åŒæ¨¡å‹ä¸ ``--version``  å‚æ•°ä¹‹é—´çš„å¯¹åº”å…³ç³»çš„å¯¹åº”å…³ç³»ã€‚

#### Situation 2.2 CLI (Huggingface version)

é€šè¿‡ä»¥ä¸‹æ–¹å¼è¿è¡ŒCLIæ¼”ç¤ºï¼š

```bash
# CogAgent
python cli_demo_hf.py --from_pretrained THUDM/cogagent-chat-hf --bf16
python cli_demo_hf.py --from_pretrained THUDM/cogagent-vqa-hf --bf16

# CogVLM
python cli_demo_hf.py --from_pretrained THUDM/cogvlm-chat-hf --bf16
python cli_demo_hf.py --from_pretrained THUDM/cogvlm-grounding-generalist --bf16
```

- å¦‚æœä½ æƒ³æ‰‹åŠ¨ä¸‹è½½æƒé‡ï¼Œä½ å¯ä»¥å°† ``--from_pretrained`` åçš„è·¯å¾„æ›¿æ¢ä¸ºæ¨¡å‹è·¯å¾„ã€‚

- ä½ å¯ä»¥å°† ``--bf16`` æ›´æ”¹ä¸º ``--fp16``, æˆ–è€… ``--quant 4``ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æ”¯æŒHuggingfaceçš„**4-bit quantization**:
    ```bash
    python cli_demo_hf.py --from_pretrained THUDM/cogvlm-chat-hf --quant 4
    ```

#### Situation 2.3 Web Demo

æˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªåŸºäºGradioçš„æœ¬åœ°ç½‘ç»œæ¼”ç¤ºã€‚é¦–å…ˆï¼Œé€šè¿‡è¿è¡Œ `pip install gradio` æ¥å®‰è£…Gradioã€‚ç„¶åä¸‹è½½å¹¶è¿›å…¥è¿™ä¸ªä»“åº“ï¼Œè¿è¡Œ `web_demo.py`ã€‚
è¯¦ç»†çš„ä½¿ç”¨æ–¹æ³•è¯·å‚è§ä¸‹ä¸€èŠ‚ï¼š

```bash
python web_demo.py --from_pretrained cogagent-chat --version chat --bf16
python web_demo.py --from_pretrained cogagent-vqa --version chat_old --bf16
python web_demo.py --from_pretrained cogvlm-chat-v1.1 --version chat_old --bf16
python web_demo.py --from_pretrained cogvlm-grounding-generalist --version base --bf16
```

ç½‘é¡µæ¼”ç¤ºçš„å›¾å½¢ç”¨æˆ·ç•Œé¢å¦‚ä¸‹ï¼š

<div align="center">
    <img src=assets/web_demo-min.png width=70% />
</div>

### é€‰é¡¹3ï¼šå¾®è°ƒ CogAgent / CogVLM

ä½ å¯èƒ½æƒ³åœ¨ä½ è‡ªå·±çš„ä»»åŠ¡ä¸­ä½¿ç”¨CogVLMï¼Œè¿™éœ€è¦ **ä¸åŒçš„è¾“å‡ºé£æ ¼æˆ–é¢†åŸŸçŸ¥è¯†**. **æ‰€æœ‰ç”¨äºå¾®è°ƒçš„ä»£ç éƒ½ä½äº  ``finetune_demo/`` ç›®å½•ä¸­ã€‚**

æˆ‘ä»¬åœ¨è¿™é‡Œæä¾›äº†ä¸€ä¸ªä½¿ç”¨loraè¿›è¡Œ **éªŒè¯ç è¯†åˆ«** çš„å¾®è°ƒç¤ºä¾‹ã€‚

1. é¦–å…ˆä¸‹è½½ [Captcha Images](https://www.kaggle.com/datasets/aadhavvignesh/captcha-images)æ•°æ®é›†ã€‚ä¸‹è½½å®Œæˆåï¼Œè§£å‹ZIPæ–‡ä»¶çš„å†…å®¹ã€‚

2. è¦åˆ›å»ºä¸€ä¸ªä»¥80/5/15çš„æ¯”ä¾‹è¿›è¡Œè®­ç»ƒ/éªŒè¯/æµ‹è¯•åˆ’åˆ†ï¼Œè¯·æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
    ```bash
    python utils/split_dataset.py
    ```

3. ä½¿ç”¨æ­¤å‘½ä»¤å¼€å§‹å¾®è°ƒï¼š

    ```bash
    bash finetune_demo/finetune_(cogagent/cogvlm)_lora.sh
    ```

4. å°†æ¨¡å‹åˆå¹¶åˆ°  `model_parallel_size=1`: (ç”¨ä½ çš„è®­ç»ƒ `MP_SIZE` æ›¿æ¢ä¸‹é¢çš„4)

    ```bash
    torchrun --standalone --nnodes=1 --nproc-per-node=4 utils/merge_model.py --version base --bf16 --from_pretrained ./checkpoints/merged_lora_(cogagent/cogvlm490/cogvlm224)
    ```

5. ä¼°ä½ çš„æ¨¡å‹çš„æ€§èƒ½ã€‚
    ```bash
    bash finetune_demo/evaluate_(cogagent/cogvlm).sh
    ```

### é€‰é¡¹4ï¼šOpenAIæ ¼å¼

We provide the same API examples as `GPT-4V`, which you can view in `openai_demo`.

1. é¦–å…ˆï¼Œå¯åŠ¨èŠ‚ç‚¹

```
python openai_demo/openai_api.py
```

2. æ¥ä¸‹æ¥ï¼Œè¿è¡Œè¯·æ±‚ç¤ºä¾‹èŠ‚ç‚¹ï¼Œè¿™æ˜¯ä¸€ä¸ªè¿ç»­å¯¹è¯çš„ä¾‹å­

```
python openai_demo/openai_api_request.py
```

3. ä½ å°†å¾—åˆ°ç±»ä¼¼äºä»¥ä¸‹çš„è¾“å‡º

```
This image showcases a tranquil natural scene with a wooden pathway leading through a field of lush green grass. In the distance, there are trees and some scattered structures, possibly houses or small buildings. The sky is clear with a few scattered clouds, suggesting a bright and sunny day.
```

### ç¡¬ä»¶éœ€æ±‚

* æ¨¡å‹æ¨ç†:

  For INT4 quantization: 1 * RTX 3090(24G)   (CogAgent takes ~ 12.6GB, CogVLM takes ~ 11GB)

  For FP16: 1 * A100(80G) or 2 * RTX 3090(24G)

* å¾®è°ƒ:

  For FP16: 4 * A100(80G) *[Recommend]* or 8* RTX 3090(24G).

### Model checkpoints

å¦‚æœä½ ä»ä»£ç ä»“åº“è¿è¡Œ `basic_demo/cli_demo*.py`ï¼Œå®ƒå°†è‡ªåŠ¨ä¸‹è½½SATæˆ–Hugging Faceçš„æƒé‡ã€‚æˆ–è€…ï¼Œä½ ä¹Ÿå¯ä»¥é€‰æ‹©æ‰‹åŠ¨ä¸‹è½½å¿…è¦çš„æƒé‡ã€‚

- CogAgent

  |   æ¨¡å‹åç§°    | è¾“å…¥åˆ†è¾¨ç‡ |                             ä»‹ç»                             | Huggingface model | SAT model |
  | :-----------: | :----: | :----------------------------------------------------------: | :------: | :-------: |
  | cogagent-chat |  1120  | CogAgentçš„èŠå¤©ç‰ˆæœ¬ã€‚æ”¯æŒGUIä»£ç†ï¼Œå¤šè½®èŠå¤©å’Œè§†è§‰å®šä½ã€‚ |  [link](https://huggingface.co/THUDM/cogagent-chat-hf)       |    [link](https://huggingface.co/THUDM/CogAgent/tree/main)       |
  | cogagent-vqa |  1120  | CogAgentçš„VQAç‰ˆæœ¬ã€‚åœ¨å•è½®è§†è§‰å¯¹è¯ä¸­å…·æœ‰æ›´å¼ºçš„èƒ½åŠ›ã€‚æ¨èç”¨äºVQAåŸºå‡†æµ‹è¯•ã€‚ |  [link](https://huggingface.co/THUDM/cogagent-vqa-hf)       |    [link](https://huggingface.co/THUDM/CogAgent/tree/main)       |

- CogVLM

  |          æ¨¡å‹åç§°            | è¾“å…¥åˆ†è¾¨ç‡ |                                               ä»‹ç»                                                | Huggingface model | SAT model |
  | :-------------------------: | :----: |:-----------------------------------------------------------------------------------------------:| :------: | :-------: |
  |         cogvlm-chat-v1.1         |  490   |                    æ”¯æŒåŒæ—¶è¿›è¡Œå¤šè½®èŠå¤©å’Œè§†è§‰é—®ç­”ï¼Œæ”¯æŒè‡ªç”±çš„æç¤ºè¯ã€‚                                                    |  [link](https://huggingface.co/THUDM/cogvlm-chat-hf)        |    [link](https://huggingface.co/THUDM/CogVLM/tree/main)        |
  |       cogvlm-base-224       |  224   |      æ–‡æœ¬-å›¾åƒé¢„è®­ç»ƒåçš„åŸå§‹æ£€æŸ¥ç‚¹ã€‚             |   [link](https://huggingface.co/THUDM/cogvlm-base-224-hf)      |     [link](https://huggingface.co/THUDM/CogVLM/tree/main)       |
  |       cogvlm-base-490       |  490   |  é€šè¿‡ä» cogvlm-base-224 è¿›è¡Œä½ç½®ç¼–ç æ’å€¼ï¼Œå°†åˆ†è¾¨ç‡æå‡åˆ°490ã€‚  |   [link](https://huggingface.co/THUDM/cogvlm-base-490-hf)      |     [link](https://huggingface.co/THUDM/CogVLM/tree/main)       |
  | cogvlm-grounding-generalist |  490   | æ­¤æ£€æŸ¥ç‚¹æ”¯æŒä¸åŒçš„è§†è§‰å®šä½ä»»åŠ¡ï¼Œä¾‹å¦‚RECï¼Œå®šä½å­—å¹•ç­‰ã€‚ |    [link](https://huggingface.co/THUDM/cogvlm-grounding-generalist-hf)     |     [link](https://huggingface.co/THUDM/CogVLM/tree/main)       |

## Introduction to CogVLM

- CogVLMæ˜¯ä¸€ä¸ªå¼ºå¤§çš„å¼€æºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€‚CogVLM-17Bæ‹¥æœ‰100äº¿çš„è§†è§‰å‚æ•°å’Œ70äº¿çš„è¯­è¨€å‚æ•°ã€‚
- CogVLM-17Båœ¨10ä¸ªç»å…¸çš„è·¨æ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€ä½³æ€§èƒ½ï¼ŒåŒ…æ‹¬ NoCaps, Flicker30k captioning, RefCOCO, RefCOCO+, RefCOCOg, Visual7W, GQA, ScienceQA, VizWiz VQA and TDIUC, å¹¶åœ¨ VQAv2, OKVQA, TextVQA, COCO å­—å¹•ç­‰æ–¹é¢æ’åç¬¬äºŒ., **è¶…è¶Šæˆ–åŒ¹æ•Œ PaLI-X 55B**. CogVLMè¿˜å¯ä»¥å’Œä½ èŠå…³äºå›¾ç‰‡çš„è¯é¢˜ã€‚ 

<div align="center">
    <img src=assets/metrics-min.png width=50% />
</div>

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹MM-VETï¼ŒPOPEï¼ŒTouchStoneçš„ç»“æœã€‚ </summary>

<table>
    <tr>
        <td>Method</td>
        <td>LLM</td>
        <td>MM-VET</td>
        <td>POPE(adversarial)</td>
        <td>TouchStone</td>
    </tr>
    <tr>
        <td>BLIP-2</td>
        <td>Vicuna-13B</td>
        <td>22.4</td>
        <td>-</td>
        <td>-</td>
    </tr>
    <tr>
        <td>Otter</td>
        <td>MPT-7B</td>
        <td>24.7</td>
        <td>-</td>
        <td>-</td>
    </tr>
    <tr>
        <td>MiniGPT4</td>
        <td>Vicuna-13B</td>
        <td>24.4</td>
        <td>70.4</td>
        <td>531.7</td>
    </tr>
    <tr>
        <td>InstructBLIP</td>
        <td>Vicuna-13B</td>
        <td>25.6</td>
        <td>77.3</td>
        <td>552.4</td>
    </tr>
    <tr>
        <td>LLaMA-Adapter v2</td>
        <td>LLaMA-7B</td>
        <td>31.4</td>
        <td>-</td>
        <td>590.1</td>
    </tr>
    <tr>
        <td>LLaVA</td>
        <td>LLaMA2-7B</td>
        <td>28.1</td>
        <td>66.3</td>
        <td>602.7</td>
    </tr>
    <tr>
        <td>mPLUG-Owl</td>
        <td>LLaMA-7B</td>
        <td>-</td>
        <td>66.8</td>
        <td>605.4</td>
    </tr>
    <tr>
        <td>LLaVA-1.5</td>
        <td>Vicuna-13B</td>
        <td>36.3</td>
        <td>84.5</td>
        <td>-</td>
    </tr>
    <tr>
        <td>Emu</td>
        <td>LLaMA-13B</td>
        <td>36.3</td>
        <td>-</td>
        <td>-</td>
    </tr>
    <tr>
        <td>Qwen-VL-Chat</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
        <td>645.2</td>
    </tr>
    <tr>
        <td>DreamLLM</td>
        <td>Vicuna-7B</td>
        <td>35.9</td>
        <td>76.5</td>
        <td>-</td>
    </tr>
    <tr>
        <td>CogVLM</td>
        <td>Vicuna-7B</td>
        <td> <b>52.8</b> </td>
        <td><b>87.6</b></td>
        <td><b>742.0</b></td>
    </tr>
</table>

</details>

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹cogvlm-grounding-generalist-v1.1çš„ç»“æœã€‚</summary>

<table>
    <tr>
        <td></td>
        <td>RefCOCO</td>
        <td></td>
        <td></td>
        <td>RefCOCO+</td>
        <td></td>
        <td></td>
        <td>RefCOCOg</td>
        <td></td>
        <td>Visual7W</td>
    </tr>
    <tr>
        <td></td>
        <td>val</td>
        <td>testA</td>
        <td>testB</td>
        <td>val</td>
        <td>testA</td>
        <td>testB</td>
        <td>val</td>
        <td>test</td>
        <td>test</td>
    </tr>
    <tr>
        <td>cogvim-grounding-generalist</td>
        <td>92.51</td>
        <td>93.95</td>
        <td>88.73</td>
        <td>87.52</td>
        <td>91.81</td>
        <td>81.43</td>
        <td>89.46</td>
        <td>90.09</td>
        <td>90.96</td>
    </tr>
    <tr>
        <td>cogvim-grounding-generalist-v1.1</td>
        <td>**92.76**</td>
        <td>**94.75**</td>
        <td>**88.99**</td>
        <td>**88.68**</td>
        <td>**92.91**</td>
        <td>**83.39**</td>
        <td>**89.75**</td>
        <td>**90.79**</td>
        <td>**91.05**</td>
    </tr>
</table>
</details>

### ç¤ºä¾‹

<!-- CogVLM is powerful for answering various types of visual questions, including **Detailed Description & Visual Question Answering**,  **Complex Counting**, **Visual Math Problem Solving**, **OCR-Free Reasonging**, **OCR-Free Visual Question Answering**, **World Knowledge**, **Referring Expression Comprehension**, **Programming with Visual Input**, **Grounding with Caption**, **Grounding Visual Question Answering**, etc. -->

* CogVLMèƒ½å¤Ÿå‡†ç¡®åœ°è¯¦ç»†æè¿°å›¾åƒï¼Œå‡ ä¹ä¸ä¼šäº§ç”Ÿå¹»è§‰ã€‚
    <details>
    <summary>ç‚¹å‡»ä»¥ä¸LLAVA-1.5å’ŒMiniGPT-4è¿›è¡Œæ¯”è¾ƒã€‚.</summary>

    <img src=assets/llava-comparison-min.png width=50% />

    </details>
    <br>

* CogVLMèƒ½ç†è§£å¹¶å›ç­”å„ç§ç±»å‹çš„é—®é¢˜ï¼Œå¹¶ä¸”æœ‰ä¸€ä¸ªè§†è§‰åŸºç¡€ç‰ˆæœ¬ã€‚

<div align="center">
    <img src=assets/pear_grounding.png width=50% />
</div>

<br>

* CogVLMæœ‰æ—¶æ¯”GPT-4V(ision)æ•è·æ›´è¯¦ç»†çš„å†…å®¹ã€‚

<div align="center">
    <img src=assets/compare-min.png width=50% />
</div>

<!-- ![compare](assets/compare.png) -->
<br> 

<details>
<summary>ç‚¹å‡»ä»¥å±•å¼€æ›´å¤šç¤ºä¾‹ã€‚</summary>

![Chat Examples](assets/chat.png)

</details>

## Introduction to CogAgent

CogAgentæ˜¯ä¸€ä¸ªåŸºäºCogVLMæ”¹è¿›çš„å¼€æºè§†è§‰è¯­è¨€æ¨¡å‹ã€‚CogAgent-18Bæ‹¥æœ‰110äº¿çš„è§†è§‰å‚æ•°å’Œ70äº¿çš„è¯­è¨€å‚æ•°ã€‚

CogAgent-18Båœ¨9ä¸ªç»å…¸çš„è·¨æ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„å…¨èƒ½æ€§èƒ½ï¼ŒåŒ…æ‹¬VQAv2ã€OK-VQã€TextVQAã€ST-VQAã€ChartQAã€infoVQAã€DocVQAã€MM-Vetå’ŒPOPEã€‚å®ƒåœ¨å¦‚AITWå’ŒMind2Webç­‰GUIæ“ä½œæ•°æ®é›†ä¸Šæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„æ¨¡å‹ã€‚

é™¤äº†CogVLMå·²æœ‰çš„æ‰€æœ‰åŠŸèƒ½ï¼ˆè§†è§‰å¤šè½®å¯¹è¯ï¼Œè§†è§‰å®šä½ï¼‰ä¹‹å¤–ï¼ŒCogAgentï¼š

1. æ”¯æŒ**æ›´é«˜åˆ†è¾¨ç‡**çš„è§†è§‰è¾“å…¥å’Œå¯¹è¯å¼é—®ç­”ã€‚å®ƒæ”¯æŒè¶…é«˜åˆ†è¾¨ç‡çš„å›¾åƒè¾“å…¥ï¼Œè¾¾åˆ°**1120x1120**ã€‚

2. **æ‹¥æœ‰è§†è§‰Agentçš„èƒ½åŠ›**ï¼Œèƒ½å¤Ÿåœ¨ä»»ä½•å›¾å½¢ç”¨æˆ·ç•Œé¢æˆªå›¾ä¸Šï¼Œä¸ºä»»ä½•ç»™å®šä»»åŠ¡è¿”å›ä¸€ä¸ªè®¡åˆ’ï¼Œä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼Œä»¥åŠå¸¦æœ‰åæ ‡çš„ç‰¹å®šæ“ä½œã€‚

3. **å¢å¼ºäº†ä¸å›¾å½¢ç”¨æˆ·ç•Œé¢ç›¸å…³çš„é—®ç­”èƒ½åŠ›**ï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†å…³äºä»»ä½•å›¾å½¢ç”¨æˆ·ç•Œé¢æˆªå›¾çš„é—®é¢˜ï¼Œä¾‹å¦‚ç½‘é¡µã€PCåº”ç”¨ã€ç§»åŠ¨åº”ç”¨ç­‰ã€‚

4. é€šè¿‡æ”¹è¿›é¢„è®­ç»ƒå’Œå¾®è°ƒï¼Œæé«˜äº†OCRç›¸å…³ä»»åŠ¡çš„èƒ½åŠ›ã€‚

<div align="center">
    <img src=assets/cogagent_function.jpg width=60% />
</div>

### GUI Agent Examples

<div align="center">
    <img src=assets/cogagent_main_demo.jpg width=90% />
</div>

## Cookbook

### Task Prompts

1. **é€šç”¨å¤šè½®å¯¹è¯**: éšä¾¿ä½ è¯´ä»€ä¹ˆ.

2. **GUIä»£ç†ä»»åŠ¡**: ä½¿ç”¨ [ä»£ç†æ¨¡æ¿](https://github.com/THUDM/CogVLM/blob/main/utils/utils/template.py#L761)å¹¶ç”¨åŒå¼•å·æ‹¬èµ·æ¥çš„ä»»åŠ¡æŒ‡ä»¤æ›¿æ¢ `\<TASK\>`ã€‚è¿™ä¸ªæŸ¥è¯¢å¯ä»¥è®©CogAgentæ¨æ–­å‡ºè®¡åˆ’å’Œä¸‹ä¸€æ­¥è¡ŒåŠ¨ã€‚å¦‚æœåœ¨æŸ¥è¯¢çš„æœ«å°¾æ·»åŠ `(with grounding)` æ¨¡å‹å°†è¿”å›ä¸€ä¸ªå¸¦æœ‰åæ ‡çš„æ­£å¼åŒ–åŠ¨ä½œè¡¨ç¤ºã€‚

ä¾‹å¦‚ï¼Œè¦è¯¢é—®æ¨¡å‹å¦‚ä½•å®Œæˆ"åœ¨å½“å‰GUIæˆªå›¾ä¸Šæœç´¢CogVLM"çš„ä»»åŠ¡ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š

1. ä»ä»£ç†æ¨¡æ¿ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªæ¨¡æ¿ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬é€‰æ‹©äº†``What steps do I need to take to <TASK>?``.

2. è¯·ç”¨åŒå¼•å·ä¸­çš„ä»»åŠ¡æŒ‡ä»¤æ›¿æ¢ï¼Œä¾‹å¦‚ï¼Œ ``What steps do I need to take to "Search for CogVLM"?``ã€‚å°†æ­¤è¾“å…¥åˆ°æ¨¡å‹ä¼šäº§ç”Ÿï¼š

> Plan: 1. Type 'CogVLM' into the Google search bar. 2. Review the search results that appear. 3. Click on a relevant
> result to read more about CogVLM or access further resources.
>
> Next Action: Move the cursor to the Google search bar, and type 'CogVLM' into it.

3. å¦‚æœåœ¨æœ«å°¾æ·»åŠ  ``(with grounding)`` å³å°†è¾“å…¥æ”¹ä¸º``What steps do I need to take to "Search for CogVLM"?(with grounding)``,é‚£ä¹ˆCogAgentçš„è¾“å‡ºå°†ä¼šæ˜¯:

> Plan: 1. Type 'CogVLM' into the Google search bar. 2. Review the search results that appear. 3. Click on a relevant
> result to read more about CogVLM or access further resources.
>
> Next Action: Move the cursor to the Google search bar, and type 'CogVLM' into it.
> Grounded Operation:[combobox] Search -> TYPE: CogVLM at the box [[212,498,787,564]]

æç¤ºï¼šå¯¹äºGUIä»£ç†ä»»åŠ¡ï¼Œå»ºè®®æ¯ä¸ªå›¾åƒåªè¿›è¡Œä¸€è½®å¯¹è¯ä»¥è·å¾—æ›´å¥½çš„ç»“æœã€‚

3. **è§†è§‰å®šä½**. Tæ”¯æŒä¸‰ç§å®šä½æ¨¡å¼ï¼š

    - å¸¦æœ‰å®šä½åæ ‡ï¼ˆè¾¹ç•Œæ¡†ï¼‰çš„å›¾åƒæè¿°ã€‚ä½¿ç”¨caption_with_boxæ¨¡æ¿ä¸­çš„ä»»ä½•æ¨¡æ¿ä½œä¸ºæ¨¡å‹è¾“å…¥ã€‚ä¾‹å¦‚:

   > Can you provide a description of the image and include the coordinates [[x0,y0,x1,y1]] for each mentioned object?

    - æ ¹æ®ç‰©ä½“çš„æè¿°è¿”å›æ¥åœ°åæ ‡ï¼ˆè¾¹ç•Œæ¡†ï¼‰ã€‚ä½¿ç”¨caption2boxæ¨¡æ¿ä¸­çš„ä»»ä½•æ¨¡æ¿ï¼Œå°† <expr> æ›¿æ¢ä¸ºç‰©ä½“çš„æè¿°ã€‚ä¾‹å¦‚:

   > Can you point out *children in blue T-shirts* in the image and provide the bounding boxes of their location?

    - æ ¹æ®è¾¹ç•Œæ¡†åæ ‡æä¾›æè¿°ã€‚ä½¿ç”¨box2captionæ¨¡æ¿ä¸­çš„æ¨¡æ¿ï¼Œå°† <objs> æ›¿æ¢ä¸ºä½ç½®åæ ‡ã€‚ä¾‹å¦‚ï¼š

   > Tell me what you see within the designated area *[[086,540,400,760]]* in the picture.

**åæ ‡æ ¼å¼:** æ¨¡å‹çš„è¾“å…¥å’Œè¾“å‡ºä¸­çš„è¾¹ç•Œæ¡†åæ ‡ä½¿ç”¨ `[[x1, y1, x2, y2]]` æ ¼å¼ï¼ŒåŸç‚¹ä½äºå·¦ä¸Šè§’ï¼Œxè½´å‘å³ï¼Œyè½´å‘ä¸‹ã€‚ (x1, y1) å’Œ (x2, y2) åˆ†åˆ«æ˜¯å·¦ä¸Šè§’å’Œå³ä¸‹è§’ï¼Œå…¶å€¼ä¸ºç›¸å¯¹åæ ‡ä¹˜ä»¥1000ï¼ˆå‰ç¼€ä¸ºé›¶ï¼Œä¸‰ä½æ•°ï¼‰ã€‚

### é€‰æ‹©é€‚åˆçš„æ¨¡å‹

ç”±äºæ¨¡å‹åŠŸèƒ½çš„å·®å¼‚ï¼Œä¸åŒçš„æ¨¡å‹ç‰ˆæœ¬å¯èƒ½ä¼šæœ‰ä¸åŒçš„æ–‡æœ¬å¤„ç†å™¨ `--version`ï¼Œè¿™æ„å‘³ç€ä½¿ç”¨çš„æç¤ºæ ¼å¼ä¼šæœ‰æ‰€ä¸åŒã€‚

|         model name          | --version |
|:---------------------------:|:---------:|
|        cogagent-chat        |   chat    |
|        cogagent-vqa         | chat_old  |
|         cogvlm-chat         | chat_old  |
|      cogvlm-chat-v1.1       | chat_old  |
| cogvlm-grounding-generalist |   base    |
|       cogvlm-base-224       |   base    |
|       cogvlm-base-490       |   base    |

### å¸¸è§é—®é¢˜

* å¦‚æœä½ åœ¨è®¿é—®huggingface.coæ—¶é‡åˆ°é—®é¢˜ï¼Œä½ å¯ä»¥æ·»åŠ  `--local_tokenizer /path/to/vicuna-7b-v1.5` æ¥åŠ è½½åˆ†è¯å™¨ã€‚
* å¦‚æœä½ åœ¨ä½¿ç”¨ğŸ”¨ [SAT](https://github.com/THUDM/SwissArmyTransformer)è‡ªåŠ¨ä¸‹è½½æ¨¡å‹æ—¶é‡åˆ°é—®é¢˜ , å°è¯•ä» ğŸ¤–[modelscope](https://www.modelscope.cn/models/ZhipuAI/CogVLM/summary) æˆ–
  ğŸ¤—[huggingface](https://huggingface.co/THUDM/CogVLM) or ğŸ’¡[wisemodel](https://www.wisemodel.cn/models/ZhipuAI/CogVLM) æ‰‹åŠ¨ä¸‹è½½ã€‚
* ä½¿ç”¨ğŸ”¨ SATä¸‹è½½æ¨¡å‹ï¼Œæ¨¡å‹å°†è¢«ä¿å­˜åˆ°é»˜è®¤ä½ç½® `~/.sat_models` ã€‚é€šè¿‡è®¾ç½®ç¯å¢ƒå˜é‡ `SAT_HOME` æ¥æ›´æ”¹é»˜è®¤ä½ç½®ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æƒ³å°†æ¨¡å‹ä¿å­˜åˆ° `/path/to/my/models` ï¼Œä½ å¯ä»¥åœ¨è¿è¡Œpythonå‘½ä»¤ä¹‹å‰è¿è¡Œ `export SAT_HOME=/path/to/my/models`ã€‚

## License

æ­¤ä»“åº“ä¸­çš„ä»£ç æ˜¯åœ¨[Apache-2.0 license](./LICENSE)çš„å¼€æºä»£ç ï¼Œè€Œä½¿ç”¨CogVLMæ¨¡å‹æƒé‡å¿…é¡»éµå®ˆ[æ¨¡å‹è®¸å¯](./MODEL_LICENSE).

## Citation & Acknowledgements

å¦‚æœä½ å‘ç°æˆ‘ä»¬çš„å·¥ä½œå¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œè¯·å¼•ç”¨ä»¥ä¸‹è®ºæ–‡
```
@misc{wang2023cogvlm,
      title={CogVLM: Visual Expert for Pretrained Language Models}, 
      author={Weihan Wang and Qingsong Lv and Wenmeng Yu and Wenyi Hong and Ji Qi and Yan Wang and Junhui Ji and Zhuoyi Yang and Lei Zhao and Xixuan Song and Jiazheng Xu and Bin Xu and Juanzi Li and Yuxiao Dong and Ming Ding and Jie Tang},
      year={2023},
      eprint={2311.03079},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{hong2023cogagent,
      title={CogAgent: A Visual Language Model for GUI Agents}, 
      author={Wenyi Hong and Weihan Wang and Qingsong Lv and Jiazheng Xu and Wenmeng Yu and Junhui Ji and Yan Wang and Zihan Wang and Yuxiao Dong and Ming Ding and Jie Tang},
      year={2023},
      eprint={2312.08914},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

```

åœ¨CogVLMçš„æŒ‡ä»¤å¾®è°ƒé˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨äº†æ¥è‡ª [MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4), [LLAVA](https://github.com/haotian-liu/LLaVA), [LRV-Instruction](https://github.com/FuxiaoLiu/LRV-Instruction), [LLaVAR](https://github.com/SALT-NLP/LLaVAR) å’Œ [Shikra](https://github.com/shikras/shikra)é¡¹ç›®çš„ä¸€äº›è‹±æ–‡å›¾åƒ-æ–‡æœ¬æ•°æ®ï¼Œä»¥åŠè®¸å¤šç»å…¸çš„è·¨æ¨¡æ€å·¥ä½œæ•°æ®é›†ã€‚æˆ‘ä»¬è¡·å¿ƒæ„Ÿè°¢ä»–ä»¬çš„è´¡çŒ®ã€‚